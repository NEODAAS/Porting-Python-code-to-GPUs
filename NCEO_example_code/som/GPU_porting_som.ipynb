{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "knowing-regulation",
   "metadata": {},
   "source": [
    "# Notebook to investigate the som.py script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-collins",
   "metadata": {},
   "source": [
    "## Running the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for running the original code:\n",
    "import random\n",
    "import sys\n",
    "import xarray as xr\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress class from original code\n",
    "class Progress(object):\n",
    "\n",
    "    def __init__(self,label,silent=False):\n",
    "        self.label = label\n",
    "        self.last_progress_frac = None\n",
    "        self.silent = silent\n",
    "\n",
    "    def report(self,msg,progress_frac):\n",
    "        if self.silent:\n",
    "            return\n",
    "        if self.last_progress_frac is None or (progress_frac - self.last_progress_frac) >= 0.01:\n",
    "            self.last_progress_frac = progress_frac\n",
    "            i = int(100*progress_frac)\n",
    "            if i > 100:\n",
    "                i = 100\n",
    "            si = i // 2\n",
    "            sys.stdout.write(\"\\r%s %s %-05s %s\" % (self.label,msg,str(i)+\"%\",\"#\"*si))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def complete(self,msg):\n",
    "        if self.silent:\n",
    "            return\n",
    "        sys.stdout.write(\"\\n%s %s\\n\" % (self.label,msg))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SelfOrganisingMap class from the original code. Includes the fit_transform function\n",
    "# which is the bit that takes the time and will be modified to try to speed up code.\n",
    "\n",
    "class SelfOrganisingMap(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood, verbose=False, seed=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.iters = iters\n",
    "        self.initial_neighbourhood = initial_neighbourhood\n",
    "        self.verbose = verbose\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "        self.learn_rate_initial = 0.5\n",
    "        self.learn_rate_final = 0.05\n",
    "\n",
    "    def get_weights(self,outputIndex):\n",
    "        return self.weights[:,outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, instances):\n",
    "        self.neighbour_limit = 0\n",
    "        self.nr_inputs = instances.shape[1]\n",
    "        self.nr_instances = instances.shape[0]\n",
    "        self.instance_mask = ~np.any(np.isnan(instances), axis=1)\n",
    "\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.nr_weights = self.nr_outputs * self.nr_inputs\n",
    "\n",
    "        self.weights = np.zeros((self.nr_inputs, self.nr_outputs))\n",
    "        for row in range(0, self.nr_inputs):\n",
    "            for col in range(0, self.nr_outputs):\n",
    "                self.weights[row, col] = self.rng.random()\n",
    "\n",
    "        p = Progress(\"SOM\",silent=not self.verbose)\n",
    "        progress_frac = 0.0\n",
    "        p.report(\"Starting\", progress_frac)\n",
    "        iteration = 0\n",
    "        while iteration < self.iters:\n",
    "            learn_rate = (1.0 - float(iteration) / float(self.iters)) \\\n",
    "                         * (self.learn_rate_initial - self.learn_rate_final) + self.learn_rate_final\n",
    "            neighbour_limit = self.initial_neighbourhood - int(\n",
    "                (float(iteration) / float((self.iters + 1))) * self.initial_neighbourhood)\n",
    "            logging.debug(\"iter=%d (of %d) / learning-rate=%f / neighbourhood=%d\"%(iteration, self.iters,\n",
    "                                                                                   learn_rate,\n",
    "                                                                                   neighbour_limit))\n",
    "            for i in range(self.nr_instances):\n",
    "                if self.instance_mask[i]:\n",
    "                    winner = self.compute_activations(instances[i, :])\n",
    "                    self.update_network(winner, instances[i, :], neighbour_limit, learn_rate)\n",
    "\n",
    "            iteration += 1\n",
    "            progress_frac = iteration/self.iters\n",
    "            p.report(\"Training neighbourhood=%d\"%(neighbour_limit), progress_frac)\n",
    "\n",
    "        p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        scores = np.zeros(shape=(self.nr_instances, 2))\n",
    "\n",
    "        for i in range(self.nr_instances):\n",
    "            if self.instance_mask[i]:\n",
    "                winner = self.coords(self.compute_activations(instances[i, :]))\n",
    "            else:\n",
    "                winner = [np.nan,np.nan]\n",
    "            scores[i,:] = np.array(winner)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_activations(self,values):\n",
    "        inarr = np.expand_dims(values, axis=1)\n",
    "        sqdiffs = (self.weights - inarr) ** 2\n",
    "        sumsdiffs = np.sum(sqdiffs, axis=0)\n",
    "        return np.argmin(sumsdiffs)\n",
    "\n",
    "    def update_network(self, winner, values, neighbour_limit, learn_rate):\n",
    "        (wx,wy) = self.coords(winner)\n",
    "        for x in range(max(0,wx-neighbour_limit),min(self.gridwidth, wx+neighbour_limit+1)):\n",
    "            for y in range(max(0, wy - neighbour_limit), min(self.gridheight, wy + neighbour_limit + 1)):\n",
    "                index = self.get_output(x, y)\n",
    "                self.weights[:,index] -= learn_rate * (self.weights[:, index]-values)\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "    def get_output(self, x, y):\n",
    "        return x + (y*self.gridwidth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to download the test data:\n",
    "#!wget https://gws-access.jasmin.ac.uk/public/nceo_uor/niall/sla_c3s_clim.nc -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next few cells contain the main code in the som.py script\n",
    "# that runs on the example file\n",
    "\n",
    "# SOM training parameters\n",
    "# we would like to be able to run gridsize=100, iters=100\n",
    "gridsize = 8\n",
    "gridheight = 8\n",
    "iters = 10\n",
    "\n",
    "initial_neighbourhood = min(2,int(gridsize/3))\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\"sla_c3s\"] # sea level anomalies averaged by month-of-year,\n",
    "                                                        # lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\",\"lon\")\n",
    "stack_sizes = (da.shape[1],da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, initial_neighbourhood, seed=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-frost",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore lat/lon dimensions and output\n",
    "a = scores.reshape(stack_sizes + (2,))\n",
    "new_dims = stack_dims + (\"som_axis\",)\n",
    "output = xr.DataArray(data=a, dims=new_dims, name=\"monthly_sla_som\")\n",
    "output.to_netcdf(\"som.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-registration",
   "metadata": {},
   "source": [
    "## Using CuPy as drop in replacement for NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SelfOrganisingMap class with small modifcation to work with CuPy:\n",
    "# wx and wy need to be converted to integers in the update_network method.\n",
    "\n",
    "class SelfOrganisingMap(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood, verbose=False, seed=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.iters = iters\n",
    "        self.initial_neighbourhood = initial_neighbourhood\n",
    "        self.verbose = verbose\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "        self.learn_rate_initial = 0.5\n",
    "        self.learn_rate_final = 0.05\n",
    "\n",
    "    def get_weights(self,outputIndex):\n",
    "        return self.weights[:,outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, instances):\n",
    "        self.neighbour_limit = 0\n",
    "        self.nr_inputs = instances.shape[1]\n",
    "        self.nr_instances = instances.shape[0]\n",
    "        self.instance_mask = ~np.any(np.isnan(instances), axis=1)\n",
    "\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.nr_weights = self.nr_outputs * self.nr_inputs\n",
    "\n",
    "        self.weights = np.zeros((self.nr_inputs, self.nr_outputs), dtype=np.float32)\n",
    "        for row in range(0, self.nr_inputs):\n",
    "            for col in range(0, self.nr_outputs):\n",
    "                self.weights[row, col] = self.rng.random()\n",
    "\n",
    "        p = Progress(\"SOM\",silent=not self.verbose)\n",
    "        progress_frac = 0.0\n",
    "        p.report(\"Starting\", progress_frac)\n",
    "        iteration = 0\n",
    "        while iteration < self.iters:\n",
    "            learn_rate = (1.0 - float(iteration) / float(self.iters)) \\\n",
    "                         * (self.learn_rate_initial - self.learn_rate_final) + self.learn_rate_final\n",
    "            neighbour_limit = self.initial_neighbourhood - int(\n",
    "                (float(iteration) / float((self.iters + 1))) * self.initial_neighbourhood)\n",
    "            logging.debug(\"iter=%d (of %d) / learning-rate=%f / neighbourhood=%d\"%(iteration, self.iters,\n",
    "                                                                                   learn_rate,\n",
    "                                                                                   neighbour_limit))\n",
    "            for i in range(self.nr_instances):\n",
    "                if self.instance_mask[i]:\n",
    "                    winner = self.compute_activations(instances[i, :])\n",
    "                    self.update_network(winner, instances[i, :], neighbour_limit, learn_rate)\n",
    "\n",
    "            iteration += 1\n",
    "            progress_frac = iteration/self.iters\n",
    "            p.report(\"Training neighbourhood=%d\"%(neighbour_limit), progress_frac)\n",
    "\n",
    "        p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        scores = np.zeros(shape=(self.nr_instances, 2))\n",
    "\n",
    "        for i in range(self.nr_instances):\n",
    "            if self.instance_mask[i]:\n",
    "                winner = self.coords(self.compute_activations(instances[i, :]))\n",
    "            else:\n",
    "                winner = [np.nan,np.nan]\n",
    "            scores[i,:] = np.array(winner)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_activations(self,values):\n",
    "        inarr = np.expand_dims(values, axis=1)\n",
    "        sqdiffs = (self.weights - inarr) ** 2\n",
    "        sumsdiffs = np.sum(sqdiffs, axis=0)\n",
    "        return np.argmin(sumsdiffs)\n",
    "\n",
    "    def update_network(self, winner, values, neighbour_limit, learn_rate):\n",
    "        (wx,wy) = self.coords(winner)\n",
    "        wx = int(wx)  ## modified\n",
    "        wy = int(wy)  ## modified\n",
    "        for x in range(max(0,wx-neighbour_limit),min(self.gridwidth, wx+neighbour_limit+1)):\n",
    "            for y in range(max(0, wy - neighbour_limit), min(self.gridheight, wy + neighbour_limit + 1)):\n",
    "                index = self.get_output(x, y)\n",
    "                self.weights[:,index] -= learn_rate * (self.weights[:, index]-values)\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "    def get_output(self, x, y):\n",
    "        return x + (y*self.gridwidth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code from the som.py script with small modification to work with CuPy\n",
    "# Because the instances array was set up using xarray, it was automatically a\n",
    "# NumPy array so I needed to convert this to a CuPy array (here cupy imported as np!)\n",
    "\n",
    "# SOM training parameters\n",
    "# we would like to be able to run gridsize=100, iters=100\n",
    "gridsize = 8\n",
    "gridheight = 8\n",
    "iters = 10\n",
    "\n",
    "initial_neighbourhood = min(2,int(gridsize/3))\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\"sla_c3s\"] # sea level anomalies averaged by month-of-year,\n",
    "                                                        # lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\",\"lon\")\n",
    "stack_sizes = (da.shape[1],da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values\n",
    "instances = np.array(instances)  # modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, initial_neighbourhood, seed=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CuPy instead of NumPy slows things down considerably, with time taken using default setting now around\n",
    "# 22mins when previously only about 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the 2 methods compare for larger gridsize?\n",
    "# Need to re-run the imports for numpy or cupy and their corresponding SelfOrganisingMap class\n",
    "# before running this cell to get results for numpy vs cupy.\n",
    "gridsize = 100\n",
    "gridheight = 100\n",
    "iters = 1  # just check for one iteration for timings comparison\n",
    "\n",
    "initial_neighbourhood = min(2,int(gridsize/3))\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\"sla_c3s\"]\n",
    "\n",
    "stack_dims = (\"lat\",\"lon\")\n",
    "stack_sizes = (da.shape[1],da.shape[2])\n",
    "\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values\n",
    "instances = np.array(instances, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CuPy:\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, initial_neighbourhood, seed=1, verbose=True)\n",
    "%time scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NumPy:\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, initial_neighbourhood, seed=1, verbose=True)\n",
    "%time scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although using NumPy for larger grids is much slower than for the smaller grids,\n",
    "# and the difference for larger grids vs smaller grids with CuPy is smaller, the time using NumPy for\n",
    "# 100 x 100 grid is still much faster.\n",
    "# So CuPy doesn't really help us here for this size of problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Using the default setup where the instances and weights arrays use float64 values gives different\n",
    "# results for the NumPy vs CuPy runs. The sum of squares are slighlty different and the differences \n",
    "# grow each time we go through the instance loop. I don't think it would make a difference to the\n",
    "# conclusions drawn, but want to have an exact comparison if possible.\n",
    "# Using float32 should also run faster on GPU so useful to see if we get any improvement here.\n",
    "# I've gone back and explicitly set the arrays to be float32 - unfortunately still getting different results\n",
    "# for the full set of instances (matches for longer when testing with subsets).\n",
    "# It also doesn't seem to have impact on timings.\n",
    "\n",
    "# Found this thread discussing same problem observed:\n",
    "# https://github.com/cupy/cupy/issues/2559\n",
    "# As I understand it, this implies the numpy result could be the one that is wrong when calculating np.sum(..., axis=0)\n",
    "# Will accept the differences and continue for now, but would need to look more closely at methods if using\n",
    "# any of this code in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-encoding",
   "metadata": {},
   "source": [
    "## Focus on BMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried a few different methods for calculating the best matching using a GPU, and \n",
    "# CuPy's ReductionKernel class seemed to be the fastest solution.\n",
    "# (Using guvectorize and cuda jit were quite complicated and did not lead to faster results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original method\n",
    "def cpu_distance(weights, inarr):\n",
    "    sqdiffs = (weights - inarr) ** 2\n",
    "    sumsdiffs = np.sum(sqdiffs, axis=0)\n",
    "    return np.argmin(sumsdiffs)\n",
    "\n",
    "# Original method, but with numba jit\n",
    "@njit\n",
    "def numba_cpu_distance(weights, inarr):\n",
    "    sqdiffs = (weights - inarr) ** 2\n",
    "    sumsdiffs = np.sum(sqdiffs, axis=0)\n",
    "    return np.argmin(sumsdiffs)\n",
    "\n",
    "# Original method, but replacing numpy with cupy\n",
    "def gpu_distance(weights, inarr):\n",
    "    sqdiffs = (weights - inarr) ** 2\n",
    "    sumsdiffs = cp.sum(sqdiffs, axis=0)\n",
    "    return cp.argmin(sumsdiffs)\n",
    "\n",
    "sqsum_kernel = cp.ReductionKernel(\n",
    "    'T x, T y',  # input params\n",
    "    'T z',  # output params\n",
    "    '(x - y) * (x - y)',  # map\n",
    "    'a + b',  # reduce\n",
    "    'z = a',  # post-reduction map\n",
    "    '0',  # identity value\n",
    "    'sqsum'  # kernel name\n",
    "    )\n",
    "\n",
    "# GPU method using ReductionKernel\n",
    "def gpu_reduction_distance(weights, inarr):\n",
    "    return cp.argmin(sqsum_kernel(weights, inarr, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make arrays with the same shape as those used in SOM code.\n",
    "weights_cp = cp.arange(n*n*12, dtype=cp.float32).reshape(12, n*n)\n",
    "inarr_cp = cp.arange(12, dtype=cp.float32).reshape(12, 1)\n",
    "\n",
    "weights_np = np.arange(n*n*12, dtype=np.float32).reshape(12, n*n)\n",
    "inarr_np = np.arange(12, dtype=np.float32).reshape(12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cpu_distance(weights_np, inarr_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "numba_cpu_distance(weights_np, inarr_np)  # run twice to ignore compile time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "gpu_distance(weights_cp, inarr_cp)  # run twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "gpu_reduction_distance(weights_cp, inarr_cp)  # run twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-charles",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create some lists containing results for different values of n:\n",
    "n_vals = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "cpu_times = []\n",
    "numba_cpu_times = []\n",
    "gpu_times = []\n",
    "gpu_reduction_times = []\n",
    "\n",
    "for n in n_vals:\n",
    "    weights_cp = cp.arange(n*n*12).reshape(12, n*n)\n",
    "    inarr_cp = cp.arange(12).reshape(12, 1)\n",
    "\n",
    "    weights_np = np.arange(n*n*12).reshape(12, n*n)\n",
    "    inarr_np = np.arange(12).reshape(12, 1)\n",
    "    \n",
    "    result = %timeit -o cpu_distance(weights_np, inarr_np)\n",
    "    cpu_times.append(result.average)\n",
    "    result = %timeit -o numba_cpu_distance(weights_np, inarr_np)\n",
    "    numba_cpu_times.append(result.average)\n",
    "    result = %timeit -o gpu_distance(weights_cp, inarr_cp)\n",
    "    gpu_times.append(result.average)\n",
    "    result = %timeit -o gpu_reduction_distance(weights_cp, inarr_cp)\n",
    "    gpu_reduction_times.append(result.average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_vals, cpu_times, label='NumPy')\n",
    "plt.plot(n_vals, numba_cpu_times, label='NumPy with Numba JIT wrapper')\n",
    "plt.plot(n_vals, gpu_times, label='CuPy')\n",
    "plt.plot(n_vals, gpu_reduction_times, label='CuPy reduction kernel')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggests it would be better to use GPU for gridsize > 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-inquiry",
   "metadata": {},
   "source": [
    "The running of the update network part may not take longer as the grid grows,\n",
    "however it would make sense to run on the GPU to avoid time consuming copies to/from\n",
    "the device to calculated the BMU.\n",
    "If this calculation takes much longer on the GPU it could wipe out any gains made by running\n",
    "the distance calculations on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-athens",
   "metadata": {},
   "source": [
    "## Update network calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal functions:\n",
    "def coords(output, gridsize):\n",
    "    return (output % gridsize, output // gridsize)\n",
    "\n",
    "def get_output(x, y, gridsize):\n",
    "    return x + (y*gridsize)\n",
    "\n",
    "def update_network(weights, winner, values, neighbour_limit, learn_rate, gridsize):\n",
    "    (wx,wy) = coords(winner, gridsize)\n",
    "    for x in range(max(0,wx-neighbour_limit),min(gridsize, wx+neighbour_limit+1)):\n",
    "        for y in range(max(0, wy - neighbour_limit), min(gridsize, wy + neighbour_limit + 1)):\n",
    "            index = get_output(x, y, gridsize)\n",
    "            weights[:,index] -= learn_rate * (weights[:, index]-values)\n",
    "\n",
    "# GPU - parallelise the loop:\n",
    "def update_network_gpu(weights, winner, values, neighbour_limit, learn_rate, gridsize):\n",
    "    (wx,wy) = coords(winner, gridsize)\n",
    "    x = cp.arange(max(0,wx-neighbour_limit),min(gridsize, wx+neighbour_limit+1))\n",
    "    y = cp.arange(max(0, wy - neighbour_limit), min(gridsize, wy + neighbour_limit + 1))\n",
    "    indices = get_output(x, y, gridsize)\n",
    "    weights[:,indices] -= learn_rate * (weights[:, indices]-values)\n",
    "    \n",
    "# original functions with numba jit:\n",
    "@njit\n",
    "def coords_jit(output, gridsize):\n",
    "    return (output % gridsize, output // gridsize)\n",
    "\n",
    "@njit\n",
    "def get_output_jit(x, y, gridsize):\n",
    "    return x + (y*gridsize)\n",
    "\n",
    "@njit\n",
    "def update_network_jit(weights, winner, values, neighbour_limit, learn_rate, gridsize):\n",
    "    (wx,wy) = coords_jit(winner, gridsize)\n",
    "    for x in range(max(0,wx-neighbour_limit),min(gridsize, wx+neighbour_limit+1)):\n",
    "        for y in range(max(0, wy - neighbour_limit), min(gridsize, wy + neighbour_limit + 1)):\n",
    "            index = get_output_jit(x, y, gridsize)\n",
    "            weights[:,index] -= learn_rate * (weights[:, index]-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "# values for CPU\n",
    "weights_np = np.arange(n*n*12, dtype=np.float32).reshape(12, n*n)\n",
    "inarr_np = np.arange(12, dtype=np.float32).reshape(12, 1)\n",
    "\n",
    "weights_new_np = weights_np.copy()\n",
    "winner = 0\n",
    "values_np = inarr_np.squeeze()\n",
    "neighbour_limit = 2\n",
    "learn_rate = 0.5\n",
    "\n",
    "# values for GPU\n",
    "weights_cp = cp.arange(n*n*12, dtype=cp.float32).reshape(12, n*n)\n",
    "inarr_cp = cp.arange(12, dtype=cp.float32).reshape(12, 1)\n",
    "weights_new_cp = weights_cp.copy()\n",
    "values_cp = inarr_cp.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "update_network(weights_new_np, winner, values_np, neighbour_limit, learn_rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "update_network_jit(weights_new_np, winner, values_np, neighbour_limit, learn_rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "update_network(weights_new_cp, winner, values_cp, neighbour_limit, learn_rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-equilibrium",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "update_network_gpu(weights_new_cp, winner, inarr_cp, neighbour_limit, learn_rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significantly slower with GPU (changing n does not affect values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-pottery",
   "metadata": {},
   "source": [
    "## Combined BMU and network update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Just CPU\n",
    "n = 100\n",
    "weights_np = np.arange(n*n*12, dtype=np.float32).reshape(12, n*n)\n",
    "inarr_np = np.arange(12, dtype=np.float32).reshape(12, 1)\n",
    "numba_cpu_distance(weights_np, inarr_np)\n",
    "update_network_jit(weights_np, winner, inarr_np.squeeze(), neighbour_limit, learn_rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Just GPU\n",
    "n = 100\n",
    "weights_cp = cp.arange(n*n*12, dtype=cp.float32).reshape(12, n*n)\n",
    "inarr_cp = cp.arange(12, dtype=cp.float32).reshape(12, 1)\n",
    "gpu_reduction_distance(weights_cp, inarr_cp)\n",
    "update_network_gpu(weights_cp, winner, inarr_cp, neighbour_limit, learn_rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Mixing GPU and CPU in the loop...\n",
    "n = 100\n",
    "weights_cp = cp.arange(n*n*12, dtype=cp.float32).reshape(12, n*n)\n",
    "inarr_cp = cp.arange(12, dtype=cp.float32).reshape(12, 1)\n",
    "np.argmin(gpu_reduction_distance(weights_cp, inarr_cp))\n",
    "weights_np = cp.asnumpy(weights_cp)\n",
    "inarr_np = cp.asnumpy(inarr_cp)\n",
    "update_network_jit(weights_np, winner, inarr_np.squeeze(), neighbour_limit, learn_rate, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are VERY variable! But it does seem like the three different options give similar timings when\n",
    "# we use n = 100, with GPU potentially giving faster results than CPU (sometimes the other way around!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a better idea, we'll apply these methods to the original problem and look at timings...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-spelling",
   "metadata": {},
   "source": [
    "## SOM 2 - using Niall's updated code that uses numba jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def find_bmu(values,weights):\n",
    "    inarr = np.expand_dims(values, axis=1)\n",
    "    sqdiffs = (weights - inarr) ** 2\n",
    "    sumsdiffs = np.sum(sqdiffs, axis=0)\n",
    "    return np.argmin(sumsdiffs)\n",
    "\n",
    "@njit\n",
    "def iterate(nr_instances,instances,instance_mask,weights,gridwidth,gridheight,neighbour_limit,learn_rate):\n",
    "    for i in range(nr_instances):\n",
    "        if instance_mask[i]:\n",
    "            winner = find_bmu(instances[i,:],weights)\n",
    "            wx = winner % gridwidth\n",
    "            wy = winner // gridwidth\n",
    "            update_network(weights, gridwidth, gridheight, wx, wy, instances[i, :], neighbour_limit,\n",
    "                           learn_rate)\n",
    "\n",
    "@njit\n",
    "def update_network(weights, gridwidth, gridheight, wx, wy, values, neighbour_limit, learn_rate):\n",
    "    for x in range(max(0, wx - neighbour_limit), min(gridwidth, wx + neighbour_limit + 1)):\n",
    "        for y in range(max(0, wy - neighbour_limit), min(gridheight, wy + neighbour_limit + 1)):\n",
    "            index = x + (y * gridwidth)\n",
    "            weights[:, index] -= learn_rate * (weights[:, index] - values)\n",
    "\n",
    "@njit\n",
    "def compute_scores(nr_instances,instance_mask,instances,weights,gridwidth):\n",
    "    scores = np.zeros(shape=(nr_instances, 2))\n",
    "    for i in range(nr_instances):\n",
    "        if instance_mask[i]:\n",
    "            bmu = find_bmu(instances[i, :],weights)\n",
    "            wx = bmu % gridwidth\n",
    "            wy = bmu // gridwidth\n",
    "        else:\n",
    "            wx = np.nan\n",
    "            wy = np.nan\n",
    "        scores[i, :] = np.array([wx,wy])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOrganisingMap(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood, verbose=False, seed=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.iters = iters\n",
    "        self.initial_neighbourhood = initial_neighbourhood\n",
    "        self.verbose = verbose\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "        self.learn_rate_initial = 0.5\n",
    "        self.learn_rate_final = 0.05\n",
    "\n",
    "    def get_weights(self,outputIndex):\n",
    "        return self.weights[:,outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, instances):\n",
    "        self.neighbour_limit = 0\n",
    "        self.nr_inputs = instances.shape[1]\n",
    "        self.nr_instances = instances.shape[0]\n",
    "        self.instance_mask = ~np.any(np.isnan(instances), axis=1)\n",
    "\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.nr_weights = self.nr_outputs * self.nr_inputs\n",
    "\n",
    "        self.weights = np.zeros((self.nr_inputs, self.nr_outputs))\n",
    "        for row in range(0, self.nr_inputs):\n",
    "            for col in range(0, self.nr_outputs):\n",
    "                self.weights[row, col] = self.rng.random()\n",
    "\n",
    "        p = Progress(\"SOM\",silent=not self.verbose)\n",
    "        progress_frac = 0.0\n",
    "        p.report(\"Starting\", progress_frac)\n",
    "        iteration = 0\n",
    "        while iteration < self.iters:\n",
    "            learn_rate = (1.0 - float(iteration) / float(self.iters)) \\\n",
    "                         * (self.learn_rate_initial - self.learn_rate_final) + self.learn_rate_final\n",
    "            neighbour_limit = self.initial_neighbourhood - int(\n",
    "                (float(iteration) / float((self.iters + 1))) * self.initial_neighbourhood)\n",
    "            logging.debug(\"iter=%d (of %d) / learning-rate=%f / neighbourhood=%d\"%(iteration, self.iters,\n",
    "                                                                                   learn_rate,\n",
    "                                                                                   neighbour_limit))\n",
    "\n",
    "            iterate(self.nr_instances,instances,self.instance_mask,self.weights,self.gridwidth,self.gridheight,neighbour_limit,learn_rate)\n",
    "\n",
    "            iteration += 1\n",
    "            progress_frac = iteration/self.iters\n",
    "            p.report(\"Training neighbourhood=%d\"%(neighbour_limit), progress_frac)\n",
    "\n",
    "        p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        return compute_scores(self.nr_instances,self.instance_mask,instances,self.weights,self.gridwidth)\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "    def get_output(self, x, y):\n",
    "        return x + (y*self.gridwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOM training parameters\n",
    "# we would like to be able to run gridsize=100, iters=100\n",
    "gridsize = 200\n",
    "gridheight = 200\n",
    "iters = 1\n",
    "\n",
    "initial_neighbourhood = min(2,int(gridsize/3))\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\"sla_c3s\"] # sea level anomalies averaged by month-of-year,\n",
    "                                                        # lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\",\"lon\")\n",
    "stack_sizes = (da.shape[1],da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, initial_neighbourhood, seed=1, verbose=True)\n",
    "import time\n",
    "start_time = time.time()\n",
    "scores = s.fit_transform(instances)\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time: %d seconds\" % (int(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-feedback",
   "metadata": {},
   "source": [
    "## Numba JIT plus GPU reduction kernel for BMU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqsum_kernel = cp.ReductionKernel(\n",
    "    'T x, T y',  # input params\n",
    "    'T z',  # output params\n",
    "    '(x - y) * (x - y)',  # map\n",
    "    'a + b',  # reduce\n",
    "    'z = a',  # post-reduction map\n",
    "    '0',  # identity value\n",
    "    'sqsum'  # kernel name\n",
    "    )\n",
    "\n",
    "def iterate(nr_instances,instances,instance_mask,weights,gridwidth,gridheight,neighbour_limit,learn_rate):\n",
    "    for i in range(nr_instances):\n",
    "        if instance_mask[i]:\n",
    "            weights = cp.array(weights)\n",
    "            inarr = cp.array(np.expand_dims(instances[i,:], axis=1))\n",
    "            np_ss = cp.asnumpy(sqsum_kernel(weights, inarr, axis=1))\n",
    "            winner = np.argmin(np_ss)\n",
    "            weights = cp.asnumpy(weights)\n",
    "            wx = winner % gridwidth\n",
    "            wy = winner // gridwidth\n",
    "            update_network(weights, gridwidth, gridheight, wx, wy, instances[i,:], neighbour_limit,\n",
    "                           learn_rate)\n",
    "\n",
    "@njit\n",
    "def update_network(weights, gridwidth, gridheight, wx, wy, values, neighbour_limit, learn_rate):\n",
    "    for x in range(max(0, wx - neighbour_limit), min(gridwidth, wx + neighbour_limit + 1)):\n",
    "        for y in range(max(0, wy - neighbour_limit), min(gridheight, wy + neighbour_limit + 1)):\n",
    "            index = x + (y * gridwidth)\n",
    "            weights[:, index] -= learn_rate * (weights[:, index] - values)\n",
    "\n",
    "#@njit\n",
    "def compute_scores(nr_instances,instance_mask,instances,weights,gridwidth):\n",
    "    scores = np.zeros(shape=(nr_instances, 2))\n",
    "    for i in range(nr_instances):\n",
    "        if instance_mask[i]:\n",
    "            weights = cp.array(weights)\n",
    "            inarr = cp.array(np.expand_dims(instances[i,:], axis=1))\n",
    "            np_ss = sqsum_kernel(weights, inarr, axis=1)\n",
    "            bmu = cp.argmin(np_ss)\n",
    "            wx = int(bmu % gridwidth)\n",
    "            wy = int(bmu // gridwidth)\n",
    "        else:\n",
    "            wx = np.nan\n",
    "            wy = np.nan\n",
    "        scores[i, :] = np.array([wx,wy])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOrganisingMap(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood, verbose=False, seed=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.iters = iters\n",
    "        self.initial_neighbourhood = initial_neighbourhood\n",
    "        self.verbose = verbose\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "        self.learn_rate_initial = 0.5\n",
    "        self.learn_rate_final = 0.05\n",
    "\n",
    "    def get_weights(self,outputIndex):\n",
    "        return self.weights[:,outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, instances):\n",
    "        self.neighbour_limit = 0\n",
    "        self.nr_inputs = instances.shape[1]\n",
    "        self.nr_instances = instances.shape[0]\n",
    "        self.instance_mask = ~np.any(np.isnan(instances), axis=1)\n",
    "\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.nr_weights = self.nr_outputs * self.nr_inputs\n",
    "\n",
    "        self.weights = np.zeros((self.nr_inputs, self.nr_outputs), dtype=np.float32)\n",
    "        for row in range(0, self.nr_inputs):\n",
    "            for col in range(0, self.nr_outputs):\n",
    "                self.weights[row, col] = self.rng.random()\n",
    "\n",
    "        p = Progress(\"SOM\",silent=not self.verbose)\n",
    "        progress_frac = 0.0\n",
    "        p.report(\"Starting\", progress_frac)\n",
    "        iteration = 0\n",
    "        while iteration < self.iters:\n",
    "            learn_rate = (1.0 - float(iteration) / float(self.iters)) \\\n",
    "                         * (self.learn_rate_initial - self.learn_rate_final) + self.learn_rate_final\n",
    "            neighbour_limit = self.initial_neighbourhood - int(\n",
    "                (float(iteration) / float((self.iters + 1))) * self.initial_neighbourhood)\n",
    "            logging.debug(\"iter=%d (of %d) / learning-rate=%f / neighbourhood=%d\"%(iteration, self.iters,\n",
    "                                                                                   learn_rate,\n",
    "                                                                                   neighbour_limit))\n",
    "\n",
    "            iterate(self.nr_instances,instances,self.instance_mask,self.weights,self.gridwidth,self.gridheight,neighbour_limit,learn_rate)\n",
    "\n",
    "            iteration += 1\n",
    "            progress_frac = iteration/self.iters\n",
    "            p.report(\"Training neighbourhood=%d\"%(neighbour_limit), progress_frac)\n",
    "\n",
    "        p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        return compute_scores(self.nr_instances,self.instance_mask,instances,self.weights,self.gridwidth)\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "    def get_output(self, x, y):\n",
    "        return x + (y*self.gridwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOM training parameters\n",
    "# we would like to be able to run gridsize=100, iters=100\n",
    "gridsize = 200\n",
    "gridheight = 200\n",
    "iters = 1\n",
    "\n",
    "initial_neighbourhood = min(2,int(gridsize/3))\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\"sla_c3s\"] # sea level anomalies averaged by month-of-year,\n",
    "                                                        # lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\",\"lon\")\n",
    "stack_sizes = (da.shape[1],da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values\n",
    "instances = np.array(instances, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, initial_neighbourhood, seed=1, verbose=True)\n",
    "import time\n",
    "start_time = time.time()\n",
    "scores = s.fit_transform(instances)\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time: %d seconds\" % (int(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-investor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "received-exercise",
   "metadata": {},
   "source": [
    "## Original code, swapping out np for cp, using reduction and parallel network update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SelfOrganisingMap class with small modifcation to work with CuPy:\n",
    "# wx and wy need to be converted to integers in the update_network method.\n",
    "# Plus using the reduction kernel to calculate sum of squares\n",
    "\n",
    "sqsum_kernel = cp.ReductionKernel(\n",
    "    'T x, T y',  # input params\n",
    "    'T z',  # output params\n",
    "    '(x - y) * (x - y)',  # map\n",
    "    'a + b',  # reduce\n",
    "    'z = a',  # post-reduction map\n",
    "    '0',  # identity value\n",
    "    'sqsum'  # kernel name\n",
    "    )\n",
    "\n",
    "class SelfOrganisingMap(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood, verbose=False, seed=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.iters = iters\n",
    "        self.initial_neighbourhood = initial_neighbourhood\n",
    "        self.verbose = verbose\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "        self.learn_rate_initial = 0.5\n",
    "        self.learn_rate_final = 0.05\n",
    "\n",
    "    def get_weights(self,outputIndex):\n",
    "        return self.weights[:,outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, instances):\n",
    "        self.neighbour_limit = 0\n",
    "        self.nr_inputs = instances.shape[1]\n",
    "        self.nr_instances = instances.shape[0]\n",
    "        self.instance_mask = ~np.any(np.isnan(instances), axis=1)\n",
    "\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.nr_weights = self.nr_outputs * self.nr_inputs\n",
    "\n",
    "        self.weights = np.zeros((self.nr_inputs, self.nr_outputs), dtype=np.float32)\n",
    "        for row in range(0, self.nr_inputs):\n",
    "            for col in range(0, self.nr_outputs):\n",
    "                self.weights[row, col] = self.rng.random()\n",
    "\n",
    "        p = Progress(\"SOM\",silent=not self.verbose)\n",
    "        progress_frac = 0.0\n",
    "        p.report(\"Starting\", progress_frac)\n",
    "        iteration = 0\n",
    "        while iteration < self.iters:\n",
    "            learn_rate = (1.0 - float(iteration) / float(self.iters)) \\\n",
    "                         * (self.learn_rate_initial - self.learn_rate_final) + self.learn_rate_final\n",
    "            neighbour_limit = self.initial_neighbourhood - int(\n",
    "                (float(iteration) / float((self.iters + 1))) * self.initial_neighbourhood)\n",
    "            logging.debug(\"iter=%d (of %d) / learning-rate=%f / neighbourhood=%d\"%(iteration, self.iters,\n",
    "                                                                                   learn_rate,\n",
    "                                                                                   neighbour_limit))\n",
    "            for i in range(self.nr_instances):\n",
    "                if self.instance_mask[i]:\n",
    "                    winner = self.compute_activations(instances[i, :])\n",
    "                    self.update_network(winner, instances[i, :], neighbour_limit, learn_rate)\n",
    "\n",
    "            iteration += 1\n",
    "            progress_frac = iteration/self.iters\n",
    "            p.report(\"Training neighbourhood=%d\"%(neighbour_limit), progress_frac)\n",
    "\n",
    "        p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        scores = np.zeros(shape=(self.nr_instances, 2))\n",
    "\n",
    "        for i in range(self.nr_instances):\n",
    "            if self.instance_mask[i]:\n",
    "                winner = self.coords(self.compute_activations(instances[i, :]))\n",
    "            else:\n",
    "                winner = [np.nan,np.nan]\n",
    "            scores[i,:] = np.array(winner)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def compute_activations(self,values):\n",
    "        inarr = np.expand_dims(values, axis=1)\n",
    "        sumsdiffs = sqsum_kernel(self.weights, inarr, axis=1)\n",
    "        return np.argmin(sumsdiffs)\n",
    "\n",
    "    def update_network(self, winner, values, neighbour_limit, learn_rate):\n",
    "        inarr = np.expand_dims(values, axis=1)        \n",
    "        (wx,wy) = self.coords(winner)\n",
    "        x = np.arange(max(0, int(wx-neighbour_limit)), min(gridsize, int(wx+neighbour_limit+1)))\n",
    "        y = np.arange(max(0, int(wy - neighbour_limit)), min(gridsize, int(wy + neighbour_limit + 1)))\n",
    "        x = np.expand_dims(x, axis=1)\n",
    "        y = np.expand_dims(y, axis=0)\n",
    "        indices = self.get_output(x, y)\n",
    "        indices = indices.flatten()\n",
    "        self.weights[:,indices] -= learn_rate * (self.weights[:, indices]-inarr)\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "    def get_output(self, x, y):\n",
    "        return x + (y*self.gridwidth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code from the som.py script with small modification to work with CuPy\n",
    "# Because the instances array was set up using xarray, it was automatically a\n",
    "# NumPy array so I needed to convert this to a CuPy array (here cupy imported as np!)\n",
    "\n",
    "# SOM training parameters\n",
    "# we would like to be able to run gridsize=100, iters=100\n",
    "gridsize = 100\n",
    "gridheight = 100\n",
    "iters = 1\n",
    "\n",
    "initial_neighbourhood = min(2,int(gridsize/3))\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\"sla_c3s\"] # sea level anomalies averaged by month-of-year,\n",
    "                                                        # lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\",\"lon\")\n",
    "stack_sizes = (da.shape[1],da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values\n",
    "instances = np.array(instances, dtype=np.float32)  # modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-kidney",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, initial_neighbourhood, seed=1, verbose=True)\n",
    "import time\n",
    "start_time = time.time()\n",
    "scores = s.fit_transform(instances)\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time: %d seconds\" % (int(end_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with grid 100 x 100 gave results along the lines of:\n",
    "# CPU njit: 1min\n",
    "# CPU njit mixed with GPU reduction kernel: 2min\n",
    "# GPU all: 3min\n",
    "\n",
    "# Testing with grid 200 x 200:\n",
    "# CPU: 4.5min\n",
    "# mix: 3.5min\n",
    "# GPU: 3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the results in this notebook it would seem that it is faster and simpler to run\n",
    "# with numba jit on the CPU for the types of grid sizes being used here.\n",
    "# If the grid size is more like 200x200 or more then we start to see a benefit of using\n",
    "# a GPU.\n",
    "# Timings above just include 1 iteration so if looking at doing ~ 100 iters, the difference are\n",
    "# going to be more significant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
