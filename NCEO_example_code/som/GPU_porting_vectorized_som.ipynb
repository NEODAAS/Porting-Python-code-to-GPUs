{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9543c09",
   "metadata": {},
   "source": [
    "# Notebook to investigate the `som3.py` script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40922398",
   "metadata": {},
   "source": [
    "## Running the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52542391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import xarray as xr\n",
    "import logging\n",
    "import numpy as np\n",
    "import numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2eb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress class from original code\n",
    "class Progress(object):\n",
    "\n",
    "    def __init__(self,label,silent=False):\n",
    "        self.label = label\n",
    "        self.last_progress_frac = None\n",
    "        self.silent = silent\n",
    "\n",
    "    def report(self,msg,progress_frac):\n",
    "        if self.silent:\n",
    "            return\n",
    "        if self.last_progress_frac is None or (progress_frac - self.last_progress_frac) >= 0.01:\n",
    "            self.last_progress_frac = progress_frac\n",
    "            i = int(100*progress_frac)\n",
    "            if i > 100:\n",
    "                i = 100\n",
    "            si = i // 2\n",
    "            sys.stdout.write(\"\\r%s %s %-05s %s\" % (self.label,msg,str(i)+\"%\",\"#\"*si))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def complete(self,msg):\n",
    "        if self.silent:\n",
    "            return\n",
    "        sys.stdout.write(\"\\n%s %s\\n\" % (self.label,msg))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f28fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bmu(instances, weights):\n",
    "    sqdiffs = (instances[:, :, None] - np.transpose(weights)) ** 2\n",
    "    sumsqdiffs = sqdiffs.sum(axis=1)\n",
    "    return sumsqdiffs.argmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091a148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(instances, weights, learn_rate, neighbourhood_lookup):\n",
    "    # winners(#instances) holds the index of the closest weight for each instance\n",
    "    winners = find_bmu(instances, weights)\n",
    "    # now find the neighbours of each winner that are also activated by each instance\n",
    "    # nhoods(#activations,2) holds the instance index and the weight index for each activation\n",
    "    nwinners = neighbourhood_lookup[winners, :]\n",
    "    nhoods = np.argwhere(nwinners)\n",
    "\n",
    "    # get the indices\n",
    "    weight_indices = nhoods[:, 1]\n",
    "    instance_indices = nhoods[:, 0]\n",
    "    fractions = nwinners[instance_indices, weight_indices]\n",
    "    # print(fractions.shape,np.min(fractions),np.max(fractions))\n",
    "\n",
    "    # get the updates\n",
    "    updates = -learn_rate * fractions[:, None] * (weights[weight_indices, :] - instances[instance_indices])\n",
    "\n",
    "    # aggregate the updates for each weight\n",
    "    numerator = np.zeros(shape=weights.shape)\n",
    "    np.add.at(numerator, weight_indices, updates)\n",
    "    denominator = np.zeros(shape=weights.shape[:1])[:, None]\n",
    "    np.add.at(denominator, weight_indices, 1)\n",
    "    denominator = np.where(numerator == 0, 1, denominator)  # fix annoying divide by zero warning\n",
    "    weight_updates = numerator / denominator\n",
    "\n",
    "    # update the weights\n",
    "    weights += weight_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954b2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(instances, weights, gridwidth, minibatch_size):\n",
    "    index = 0\n",
    "    nr_instances = instances.shape[0]\n",
    "    batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "    bmus = np.zeros(shape=(nr_instances,), dtype=int)\n",
    "    while index < nr_instances:\n",
    "        last_index = min(index + batch_size, nr_instances)\n",
    "        bmus[index:last_index] = find_bmu(instances[index:last_index], weights)\n",
    "        index += batch_size\n",
    "    scores = np.vstack([bmus % gridwidth, bmus // gridwidth])\n",
    "    return np.transpose(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07034266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOrganisingMap(object):\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood=None, verbose=False, seed=None,\n",
    "                 minibatch_size=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.iters = iters\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        self.initial_neighbourhood = initial_neighbourhood if initial_neighbourhood else int(gridsize / 3)\n",
    "        self.verbose = verbose\n",
    "        self.seed = seed\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "\n",
    "        self.learn_rate_initial = 0.01\n",
    "        self.learn_rate_final = 0.001\n",
    "        self.neighbourhood_lookup = np.zeros(shape=(self.initial_neighbourhood + 1, self.nr_outputs, self.nr_outputs))\n",
    "\n",
    "        # for each neighbourhood size 0,1,...initial_neighbourhood\n",
    "        # build a lookup table where the fraction at neighbourhood_lookup[n,o1,o2]\n",
    "        # indicates if (and how much) weight at index o2 is a neighbour of the weight at index o1 in neighbourhood size n\n",
    "        # use 1 and 0 for a binary mask, or between -1.0 and 1.0 for a varying mask\n",
    "        for neighbourhood in range(0, self.initial_neighbourhood + 1):\n",
    "            nsq = neighbourhood ** 2\n",
    "            for i in range(self.nr_outputs):\n",
    "                ix, iy = self.coords(i)\n",
    "                for j in range(self.nr_outputs):\n",
    "                    jx, jy = self.coords(j)\n",
    "                    sqdist = (jx - ix) ** 2 + (jy - iy) ** 2\n",
    "                    self.neighbourhood_lookup[neighbourhood, i, j] = 1 if sqdist <= nsq else 0\n",
    "\n",
    "    def get_weights(self, outputIndex):\n",
    "        return self.weights[:, outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, original_instances):\n",
    "\n",
    "        # mask out instances containing NaNs and remove them\n",
    "        instance_mask = ~np.any(np.isnan(original_instances), axis=1)\n",
    "        nr_original_instances = original_instances.shape[0]\n",
    "        valid_instances = original_instances[instance_mask, :]\n",
    "\n",
    "        # randomly re-shuffle the remaining instances.\n",
    "        # TODO consider reshuffling after every iteration\n",
    "        instances = np.copy(valid_instances)\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        rng.shuffle(instances)\n",
    "\n",
    "        nr_inputs = instances.shape[1]\n",
    "        nr_instances = instances.shape[0]\n",
    "\n",
    "        weights = np.zeros((self.nr_outputs, nr_inputs))\n",
    "        for output_idx in range(0, self.nr_outputs):\n",
    "            weights[output_idx, :] = instances[self.rng.choice(range(0, nr_instances)), :]\n",
    "\n",
    "        p = Progress(\"SOM\", silent=not self.verbose)\n",
    "        progress_frac = 0.0\n",
    "        p.report(\"Starting\", progress_frac)\n",
    "\n",
    "        for iteration in range(self.iters):\n",
    "            # reduce the learning rate and neighbourhood size linearly as training progresses\n",
    "            learn_rate = self.learn_rate_initial - (self.learn_rate_initial - self.learn_rate_final) * (\n",
    "                        (iteration + 1) / self.iters)\n",
    "            neighbour_limit = round(self.initial_neighbourhood * (1 - (iteration + 1) / self.iters))\n",
    "            neighbourhood_mask = self.neighbourhood_lookup[neighbour_limit, :, :]\n",
    "            batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "\n",
    "            index = 0\n",
    "            while index < nr_instances:\n",
    "                last_index = min(index + batch_size, nr_instances)\n",
    "                train_batch(instances[index:last_index, :], weights, learn_rate, neighbourhood_mask)\n",
    "                index += batch_size\n",
    "\n",
    "            progress_frac = iteration / self.iters\n",
    "            p.report(\"Training neighbourhood=%d\" % (neighbour_limit), progress_frac)\n",
    "\n",
    "        p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        # compute final scores from the trained weights\n",
    "        valid_scores = compute_scores(valid_instances, weights, self.gridwidth, self.minibatch_size)\n",
    "\n",
    "        # restore the results into the same order as the input array\n",
    "        scores = np.zeros(shape=(nr_original_instances, 2))\n",
    "        scores[:, :] = np.nan\n",
    "        scores[instance_mask, :] = valid_scores\n",
    "        return scores\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "    def get_output(self, x, y):\n",
    "        return x + (y * self.gridwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608c1aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "Elapsed time: 44 seconds\n"
     ]
    }
   ],
   "source": [
    "gridsize = 16\n",
    "gridheight = 16\n",
    "iters = 10\n",
    "minibatch_size = 1000  # the max number of instances passed in each call to train_batch\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\n",
    "    \"sla_c3s\"]  # sea level anomalies averaged by month-of-year, lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\", \"lon\")\n",
    "stack_sizes = (da.shape[1], da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values\n",
    "\n",
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=minibatch_size)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "scores = s.fit_transform(instances)\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time: %d seconds\" % (int(end_time - start_time)))\n",
    "\n",
    "# restore lat/lon dimensions and output\n",
    "a = scores.reshape(stack_sizes + (2,))\n",
    "new_dims = stack_dims + (\"som_axis\",)\n",
    "output = xr.DataArray(data=a, dims=new_dims, name=\"monthly_sla_som\")\n",
    "output.to_netcdf(\"som.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e4bf7",
   "metadata": {},
   "source": [
    "This already runs quite fast on the CPU using the new batch approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edcab0",
   "metadata": {},
   "source": [
    "## Using CuPy as drop in replacement for NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c77a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "\n",
    "def find_bmu(instances, weights):\n",
    "    sqdiffs = (instances[:, :, None] - np.transpose(weights)) ** 2\n",
    "    sumsqdiffs = sqdiffs.sum(axis=1)\n",
    "    return sumsqdiffs.argmin(axis=1)\n",
    "\n",
    "def train_batch(instances, weights, learn_rate, neighbourhood_lookup):\n",
    "    # winners(#instances) holds the index of the closest weight for each instance\n",
    "    winners = find_bmu(instances, weights)\n",
    "    # now find the neighbours of each winner that are also activated by each instance\n",
    "    # nhoods(#activations,2) holds the instance index and the weight index for each activation\n",
    "    nwinners = neighbourhood_lookup[winners, :]\n",
    "    nhoods = np.argwhere(nwinners)\n",
    "\n",
    "    # get the indices\n",
    "    weight_indices = nhoods[:, 1]\n",
    "    instance_indices = nhoods[:, 0]\n",
    "    fractions = nwinners[instance_indices, weight_indices]\n",
    "    # print(fractions.shape,np.min(fractions),np.max(fractions))\n",
    "\n",
    "    # get the updates\n",
    "    updates = -learn_rate * fractions[:, None] * (weights[weight_indices, :] - instances[instance_indices])\n",
    "\n",
    "    # aggregate the updates for each weight\n",
    "    numerator = np.zeros(shape=weights.shape)\n",
    "    #np.add.at(numerator, weight_indices, updates)     #### change required here for cupy - no cp.add.at\n",
    "    numerator[weight_indices] = numerator[weight_indices] + updates\n",
    "    denominator = np.zeros(shape=weights.shape[:1])[:, None]\n",
    "    #np.add.at(denominator, weight_indices, 1)     #### change required here for cupy - no cp.add.at\n",
    "    denominator[weight_indices] = denominator[weight_indices] + 1\n",
    "    denominator = np.where(numerator == 0, 1, denominator)  # fix annoying divide by zero warning\n",
    "    weight_updates = numerator / denominator\n",
    "\n",
    "    # update the weights\n",
    "    weights += weight_updates\n",
    "    \n",
    "def compute_scores(instances, weights, gridwidth, minibatch_size):\n",
    "    index = 0\n",
    "    nr_instances = instances.shape[0]\n",
    "    batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "    bmus = np.zeros(shape=(nr_instances,), dtype=int)\n",
    "    while index < nr_instances:\n",
    "        last_index = min(index + batch_size, nr_instances)\n",
    "        bmus[index:last_index] = find_bmu(instances[index:last_index], weights)\n",
    "        index += batch_size\n",
    "    scores = np.vstack([bmus % gridwidth, bmus // gridwidth])\n",
    "    return np.transpose(scores)\n",
    "\n",
    "class SelfOrganisingMap(object):\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood=None, verbose=False, seed=None,\n",
    "                 minibatch_size=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.iters = iters\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        self.initial_neighbourhood = initial_neighbourhood if initial_neighbourhood else int(gridsize / 3)\n",
    "        self.verbose = verbose\n",
    "        self.seed = seed\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "\n",
    "        self.learn_rate_initial = 0.01\n",
    "        self.learn_rate_final = 0.001\n",
    "        self.neighbourhood_lookup = np.zeros(shape=(self.initial_neighbourhood + 1, self.nr_outputs, self.nr_outputs))\n",
    "\n",
    "        # for each neighbourhood size 0,1,...initial_neighbourhood\n",
    "        # build a lookup table where the fraction at neighbourhood_lookup[n,o1,o2]\n",
    "        # indicates if (and how much) weight at index o2 is a neighbour of the weight at index o1 in neighbourhood size n\n",
    "        # use 1 and 0 for a binary mask, or between -1.0 and 1.0 for a varying mask\n",
    "        for neighbourhood in range(0, self.initial_neighbourhood + 1):\n",
    "            nsq = neighbourhood ** 2\n",
    "            for i in range(self.nr_outputs):\n",
    "                ix, iy = self.coords(i)\n",
    "                for j in range(self.nr_outputs):\n",
    "                    jx, jy = self.coords(j)\n",
    "                    sqdist = (jx - ix) ** 2 + (jy - iy) ** 2\n",
    "                    self.neighbourhood_lookup[neighbourhood, i, j] = 1 if sqdist <= nsq else 0\n",
    "\n",
    "#     def get_weights(self, outputIndex):\n",
    "#         return self.weights[:, outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, original_instances):\n",
    "\n",
    "        # mask out instances containing NaNs and remove them\n",
    "        instance_mask = ~np.any(np.isnan(original_instances), axis=1)\n",
    "        nr_original_instances = original_instances.shape[0]\n",
    "        valid_instances = original_instances[instance_mask, :]\n",
    "\n",
    "        # randomly re-shuffle the remaining instances.\n",
    "        # TODO consider reshuffling after every iteration\n",
    "        instances = np.copy(valid_instances)\n",
    "        #rng = np.random.default_rng(seed=self.seed)     #### change required here for cupy - no cp.random.default_rng\n",
    "        #rng.shuffle(instances)\n",
    "        np.random.shuffle(instances)  # will change every time\n",
    "\n",
    "        nr_inputs = instances.shape[1]\n",
    "        nr_instances = instances.shape[0]\n",
    "\n",
    "        weights = np.zeros((self.nr_outputs, nr_inputs))\n",
    "        for output_idx in range(0, self.nr_outputs):\n",
    "            weights[output_idx, :] = instances[self.rng.choice(range(0, nr_instances)), :]\n",
    "\n",
    "        p = Progress(\"SOM\", silent=not self.verbose)\n",
    "        progress_frac = 0.0\n",
    "        p.report(\"Starting\", progress_frac)\n",
    "\n",
    "        for iteration in range(self.iters):\n",
    "            # reduce the learning rate and neighbourhood size linearly as training progresses\n",
    "            learn_rate = self.learn_rate_initial - (self.learn_rate_initial - self.learn_rate_final) * (\n",
    "                        (iteration + 1) / self.iters)\n",
    "            neighbour_limit = round(self.initial_neighbourhood * (1 - (iteration + 1) / self.iters))\n",
    "            neighbourhood_mask = self.neighbourhood_lookup[neighbour_limit, :, :]\n",
    "            batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "\n",
    "            index = 0\n",
    "            while index < nr_instances:\n",
    "                last_index = min(index + batch_size, nr_instances)\n",
    "                train_batch(instances[index:last_index, :], weights, learn_rate, neighbourhood_mask)\n",
    "                index += batch_size\n",
    "\n",
    "            progress_frac = iteration / self.iters\n",
    "            p.report(\"Training neighbourhood=%d\" % (neighbour_limit), progress_frac)\n",
    "\n",
    "        p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        # compute final scores from the trained weights\n",
    "        valid_scores = compute_scores(valid_instances, weights, self.gridwidth, self.minibatch_size)\n",
    "\n",
    "        # restore the results into the same order as the input array\n",
    "        scores = np.zeros(shape=(nr_original_instances, 2))\n",
    "        scores[:, :] = np.nan\n",
    "        scores[instance_mask, :] = valid_scores\n",
    "        return scores\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "#     def get_output(self, x, y):\n",
    "#         return x + (y * self.gridwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddca0107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "Elapsed time: 1 seconds\n"
     ]
    }
   ],
   "source": [
    "gridsize = 16\n",
    "gridheight = 16\n",
    "iters = 10\n",
    "minibatch_size = 1000  # the max number of instances passed in each call to train_batch\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\n",
    "    \"sla_c3s\"]  # sea level anomalies averaged by month-of-year, lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\", \"lon\")\n",
    "stack_sizes = (da.shape[1], da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values\n",
    "instances = np.asarray(instances)    ### change required here - need to make instances a cupy array\n",
    "\n",
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=minibatch_size)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "scores = s.fit_transform(instances)\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time: %d seconds\" % (int(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd73a5",
   "metadata": {},
   "source": [
    "See a good speed up with very minimal changes:\n",
    "* Needed to convert instances array to cp.ndarray before passing to `fit_transform()`,\n",
    "* needed to swap out the code that used the `np.add.at` method,\n",
    "* needed to swap out the code that used the `np.random.default_rng` method - this means the results change every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3412e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "Elapsed time: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "gridsize = 16\n",
    "gridheight = 16\n",
    "iters = 10\n",
    "minibatch_size = None#1000  # the max number of instances passed in each call to train_batch\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "da = xr.open_dataset(\"data/sla_c3s_clim.nc\")[\n",
    "    \"sla_c3s\"]  # sea level anomalies averaged by month-of-year, lat and lon cell\n",
    "\n",
    "stack_dims = (\"lat\", \"lon\")\n",
    "stack_sizes = (da.shape[1], da.shape[2])\n",
    "\n",
    "# each (lat,lon) position becomes an independent case\n",
    "# flatten lat and lon dimensions and transpose to arrange by (ncases, time)\n",
    "# where ncases = nlat*nlon\n",
    "instances = da.stack(case=stack_dims).transpose(\"case\", \"month\").values\n",
    "instances = np.asarray(instances)\n",
    "\n",
    "# run SOM to reduce time dimension from 12 to 2\n",
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=minibatch_size)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "scores = s.fit_transform(instances)\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time: %d seconds\" % (int(end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a451f5f",
   "metadata": {},
   "source": [
    "Changing minibatch size to None above shows timing of 0 seconds. Let's check a few setting with more precise timings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f17eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "450 ms ± 4.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=1000)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f74f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "SOM Training neighbourhood=0 90%   #############################################\n",
      "SOM SOM Training Complete\n",
      "447 ms ± 2.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=None)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0fcb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can comment out the progress reporting though it doesn't affect timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ce147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464 ms ± 1.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=1000)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080dc8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 ms ± 3.08 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=None)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f16d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing with minibatch_size shows little difference in timings, but if you make them small enough it slows things down a little..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d58cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 ms ± 4.19 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=50)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7de7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "900eee8a",
   "metadata": {},
   "source": [
    "## Original code, swapping out np for cp, using reduction\n",
    "\n",
    "Next check if we get any improvements using the reduction kernel class in bmu calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0169614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "\n",
    "# def find_bmu(instances, weights):\n",
    "#     print(instances.shape, weights.shape)\n",
    "#     sqdiffs = (instances[:, :, None] - np.transpose(weights)) ** 2\n",
    "#     sumsqdiffs = sqdiffs.sum(axis=1)\n",
    "#     return sumsqdiffs.argmin(axis=1)\n",
    "\n",
    "sqsum_kernel = np.ReductionKernel(\n",
    "    'T x, T y',  # input params\n",
    "    'T z',  # output params\n",
    "    '(x - y) * (x - y)',  # map\n",
    "    'a + b',  # reduce\n",
    "    'z = a',  # post-reduction map\n",
    "    '0',  # identity value\n",
    "    'sqsum'  # kernel name\n",
    "    )\n",
    "\n",
    "def find_bmu(instances, weights):\n",
    "    inarr = instances[:, :, None]\n",
    "    sumsdiffs = sqsum_kernel(inarr, weights.T, axis=1)\n",
    "    return np.argmin(sumsdiffs, axis=1)  \n",
    "\n",
    "\n",
    "def train_batch(instances, weights, learn_rate, neighbourhood_lookup):\n",
    "    # winners(#instances) holds the index of the closest weight for each instance\n",
    "    winners = find_bmu(instances, weights)\n",
    "    # now find the neighbours of each winner that are also activated by each instance\n",
    "    # nhoods(#activations,2) holds the instance index and the weight index for each activation\n",
    "    nwinners = neighbourhood_lookup[winners, :]\n",
    "    nhoods = np.argwhere(nwinners)\n",
    "\n",
    "    # get the indices\n",
    "    weight_indices = nhoods[:, 1]\n",
    "    instance_indices = nhoods[:, 0]\n",
    "    fractions = nwinners[instance_indices, weight_indices]\n",
    "    # print(fractions.shape,np.min(fractions),np.max(fractions))\n",
    "\n",
    "    # get the updates\n",
    "    updates = -learn_rate * fractions[:, None] * (weights[weight_indices, :] - instances[instance_indices])\n",
    "\n",
    "    # aggregate the updates for each weight\n",
    "    numerator = np.zeros(shape=weights.shape)\n",
    "    numerator[weight_indices] = numerator[weight_indices] + updates\n",
    "    denominator = np.zeros(shape=weights.shape[:1])[:, None]\n",
    "    denominator[weight_indices] = denominator[weight_indices] + 1\n",
    "    denominator = np.where(numerator == 0, 1, denominator)  # fix annoying divide by zero warning\n",
    "    weight_updates = numerator / denominator\n",
    "\n",
    "    # update the weights\n",
    "    weights += weight_updates\n",
    "    \n",
    "def compute_scores(instances, weights, gridwidth, minibatch_size):\n",
    "    index = 0\n",
    "    nr_instances = instances.shape[0]\n",
    "    batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "    bmus = np.zeros(shape=(nr_instances,), dtype=int)\n",
    "    while index < nr_instances:\n",
    "        last_index = min(index + batch_size, nr_instances)\n",
    "        bmus[index:last_index] = find_bmu(instances[index:last_index], weights)\n",
    "        index += batch_size\n",
    "    scores = np.vstack([bmus % gridwidth, bmus // gridwidth])\n",
    "    return np.transpose(scores)\n",
    "\n",
    "class SelfOrganisingMap(object):\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood=None, verbose=False, seed=None,\n",
    "                 minibatch_size=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.iters = iters\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        self.initial_neighbourhood = initial_neighbourhood if initial_neighbourhood else int(gridsize / 3)\n",
    "        self.verbose = verbose\n",
    "        self.seed = seed\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "\n",
    "        self.learn_rate_initial = 0.01\n",
    "        self.learn_rate_final = 0.001\n",
    "        self.neighbourhood_lookup = np.zeros(shape=(self.initial_neighbourhood + 1, self.nr_outputs, self.nr_outputs))\n",
    "\n",
    "        # for each neighbourhood size 0,1,...initial_neighbourhood\n",
    "        # build a lookup table where the fraction at neighbourhood_lookup[n,o1,o2]\n",
    "        # indicates if (and how much) weight at index o2 is a neighbour of the weight at index o1 in neighbourhood size n\n",
    "        # use 1 and 0 for a binary mask, or between -1.0 and 1.0 for a varying mask\n",
    "        for neighbourhood in range(0, self.initial_neighbourhood + 1):\n",
    "            nsq = neighbourhood ** 2\n",
    "            for i in range(self.nr_outputs):\n",
    "                ix, iy = self.coords(i)\n",
    "                for j in range(self.nr_outputs):\n",
    "                    jx, jy = self.coords(j)\n",
    "                    sqdist = (jx - ix) ** 2 + (jy - iy) ** 2\n",
    "                    self.neighbourhood_lookup[neighbourhood, i, j] = 1 if sqdist <= nsq else 0\n",
    "\n",
    "#     def get_weights(self, outputIndex):\n",
    "#         return self.weights[:, outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, original_instances):\n",
    "\n",
    "        # mask out instances containing NaNs and remove them\n",
    "        instance_mask = ~np.any(np.isnan(original_instances), axis=1)\n",
    "        nr_original_instances = original_instances.shape[0]\n",
    "        valid_instances = original_instances[instance_mask, :]\n",
    "\n",
    "        # randomly re-shuffle the remaining instances.\n",
    "        # TODO consider reshuffling after every iteration\n",
    "        instances = np.copy(valid_instances)\n",
    "        #rng = np.random.default_rng(seed=self.seed)\n",
    "        #rng.shuffle(instances)\n",
    "        np.random.shuffle(instances)  # will change every time\n",
    "\n",
    "        nr_inputs = instances.shape[1]\n",
    "        nr_instances = instances.shape[0]\n",
    "\n",
    "        weights = np.zeros((self.nr_outputs, nr_inputs))\n",
    "        for output_idx in range(0, self.nr_outputs):\n",
    "            weights[output_idx, :] = instances[self.rng.choice(range(0, nr_instances)), :]\n",
    "\n",
    "#         p = Progress(\"SOM\", silent=not self.verbose)\n",
    "#         progress_frac = 0.0\n",
    "#         p.report(\"Starting\", progress_frac)\n",
    "\n",
    "        for iteration in range(self.iters):\n",
    "            # reduce the learning rate and neighbourhood size linearly as training progresses\n",
    "            learn_rate = self.learn_rate_initial - (self.learn_rate_initial - self.learn_rate_final) * (\n",
    "                        (iteration + 1) / self.iters)\n",
    "            neighbour_limit = round(self.initial_neighbourhood * (1 - (iteration + 1) / self.iters))\n",
    "            neighbourhood_mask = self.neighbourhood_lookup[neighbour_limit, :, :]\n",
    "            batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "\n",
    "            index = 0\n",
    "            while index < nr_instances:\n",
    "                last_index = min(index + batch_size, nr_instances)\n",
    "                train_batch(instances[index:last_index, :], weights, learn_rate, neighbourhood_mask)\n",
    "                index += batch_size\n",
    "\n",
    "#             progress_frac = iteration / self.iters\n",
    "#             p.report(\"Training neighbourhood=%d\" % (neighbour_limit), progress_frac)\n",
    "\n",
    "#         p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        # compute final scores from the trained weights\n",
    "        valid_scores = compute_scores(valid_instances, weights, self.gridwidth, self.minibatch_size)\n",
    "\n",
    "        # restore the results into the same order as the input array\n",
    "        scores = np.zeros(shape=(nr_original_instances, 2))\n",
    "        scores[:, :] = np.nan\n",
    "        scores[instance_mask, :] = valid_scores\n",
    "        return scores\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "#     def get_output(self, x, y):\n",
    "#         return x + (y * self.gridwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ecad518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (using settings already run above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ab08eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 ms ± 384 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=1000)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb925ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 ms ± 313 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=None)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0057c3e",
   "metadata": {},
   "source": [
    "Even faster than simple CuPy swap, though requires a little more effort and brings in differences from running on CPU. Simple replacement of NumPy with CuPy means you could run on GPU by changing 1 line. If it became important to reduce the timings further then using the reduction kernel would be useful.\n",
    "\n",
    "This batch based approach is much better suited to running on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a00400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5e0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "399851bb",
   "metadata": {},
   "source": [
    "# Updates...\n",
    "\n",
    "In above examples the `np.add.at` commands were replaced due to there being no `cupy.add.at` available, e.g.\n",
    "```python\n",
    "np.add.at(numerator, weight_indices, updates)\n",
    "```\n",
    "replaced with\n",
    "```python\n",
    "numerator[weight_indices] = numerator[weight_indices] + updates\n",
    "```\n",
    "However `weight_indices` may contain repeated index values and this replacement code does not account for this (will only increment repeated elements once). So we should use an alternative to maintain the original functionality.\n",
    "\n",
    "Cupy has the method `cupyx.scatter_add` which behaves exactly as `np.add.at` (https://docs.cupy.dev/en/stable/reference/generated/cupyx.scatter_add.html) so can use:\n",
    "```python\n",
    "cupyx.scatter_add(numerator, weight_indices, updates)\n",
    "```\n",
    "\n",
    "Another change we needed to make when using CuPy as drop in replacement for NumPy was to replace this:\n",
    "```python\n",
    "instances = np.copy(valid_instances)\n",
    "rng = np.random.default_rng(seed=self.seed)\n",
    "rng.shuffle(instances)\n",
    "```\n",
    "\n",
    "with this:\n",
    "```python\n",
    "instances = np.copy(valid_instances)\n",
    "np.random.shuffle(instances)\n",
    "```\n",
    "\n",
    "because there was no `cupy.random.default_rng`.\n",
    "\n",
    "At cupy version 9 and up, there is `cupy.random.default_rng`, however this generator object doesn't have the shuffle method. CuPy and the underlying libraries are constantly being updated so it is likely that this option will be added at some point!\n",
    "\n",
    "In the meantime we can get around it with setting a random seed then using shuffling:\n",
    "```python\n",
    "instances = np.copy(valid_instances)\n",
    "np.random.seed(seed=self.seed)\n",
    "np.random.shuffle(instances)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3430d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import cupyx\n",
    "\n",
    "# def find_bmu(instances, weights):\n",
    "#     print(instances.shape, weights.shape)\n",
    "#     sqdiffs = (instances[:, :, None] - np.transpose(weights)) ** 2\n",
    "#     sumsqdiffs = sqdiffs.sum(axis=1)\n",
    "#     return sumsqdiffs.argmin(axis=1)\n",
    "\n",
    "sqsum_kernel = np.ReductionKernel(\n",
    "    'T x, T y',  # input params\n",
    "    'T z',  # output params\n",
    "    '(x - y) * (x - y)',  # map\n",
    "    'a + b',  # reduce\n",
    "    'z = a',  # post-reduction map\n",
    "    '0',  # identity value\n",
    "    'sqsum'  # kernel name\n",
    "    )\n",
    "\n",
    "def find_bmu(instances, weights):\n",
    "    inarr = instances[:, :, None]\n",
    "    sumsdiffs = sqsum_kernel(inarr, weights.T, axis=1)\n",
    "    return np.argmin(sumsdiffs, axis=1)  \n",
    "\n",
    "\n",
    "def train_batch(instances, weights, learn_rate, neighbourhood_lookup):\n",
    "    # winners(#instances) holds the index of the closest weight for each instance\n",
    "    winners = find_bmu(instances, weights)\n",
    "    # now find the neighbours of each winner that are also activated by each instance\n",
    "    # nhoods(#activations,2) holds the instance index and the weight index for each activation\n",
    "    nwinners = neighbourhood_lookup[winners, :]\n",
    "    nhoods = np.argwhere(nwinners)\n",
    "\n",
    "    # get the indices\n",
    "    weight_indices = nhoods[:, 1]\n",
    "    instance_indices = nhoods[:, 0]\n",
    "    fractions = nwinners[instance_indices, weight_indices]\n",
    "    # print(fractions.shape,np.min(fractions),np.max(fractions))\n",
    "\n",
    "    # get the updates\n",
    "    updates = -learn_rate * fractions[:, None] * (weights[weight_indices, :] - instances[instance_indices])\n",
    "\n",
    "    # aggregate the updates for each weight\n",
    "    numerator = np.zeros(shape=weights.shape)\n",
    "    cupyx.scatter_add(numerator, weight_indices, updates)\n",
    "    denominator = np.zeros(shape=weights.shape[:1])[:, None]\n",
    "    cupyx.scatter_add(denominator, weight_indices, 1)\n",
    "    denominator = np.where(numerator == 0, 1, denominator)  # fix annoying divide by zero warning\n",
    "    weight_updates = numerator / denominator\n",
    "\n",
    "    # update the weights\n",
    "    weights += weight_updates\n",
    "    \n",
    "def compute_scores(instances, weights, gridwidth, minibatch_size):\n",
    "    index = 0\n",
    "    nr_instances = instances.shape[0]\n",
    "    batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "    bmus = np.zeros(shape=(nr_instances,), dtype=int)\n",
    "    while index < nr_instances:\n",
    "        last_index = min(index + batch_size, nr_instances)\n",
    "        bmus[index:last_index] = find_bmu(instances[index:last_index], weights)\n",
    "        index += batch_size\n",
    "    scores = np.vstack([bmus % gridwidth, bmus // gridwidth])\n",
    "    return np.transpose(scores)\n",
    "\n",
    "class SelfOrganisingMap(object):\n",
    "    \"\"\"\n",
    "    Train Self Organising Map (SOM) with cells arranged in a 2-dimensional rectangular layout\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iters : int\n",
    "        the number of training iterations to use when training the SOM\n",
    "    gridwidth : int\n",
    "        number of cells across the grid\n",
    "    gridheight : int\n",
    "        number of cells down the grid\n",
    "    initial_neighbourhood : int\n",
    "        the initial neighbourhood size\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    verbose : bool\n",
    "        whether to print progress messages\n",
    "    seed : int\n",
    "        random seed - set to produce repeatable results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gridwidth, gridheight, iters, initial_neighbourhood=None, verbose=False, seed=None,\n",
    "                 minibatch_size=None):\n",
    "        self.gridheight = gridheight\n",
    "        self.gridwidth = gridwidth\n",
    "        self.nr_outputs = self.gridwidth * self.gridheight\n",
    "        self.iters = iters\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        self.initial_neighbourhood = initial_neighbourhood if initial_neighbourhood else int(gridsize / 3)\n",
    "        self.verbose = verbose\n",
    "        self.seed = seed\n",
    "        self.rng = random.Random()\n",
    "        if seed:\n",
    "            self.rng.seed(seed)\n",
    "\n",
    "        self.learn_rate_initial = 0.01\n",
    "        self.learn_rate_final = 0.001\n",
    "        self.neighbourhood_lookup = np.zeros(shape=(self.initial_neighbourhood + 1, self.nr_outputs, self.nr_outputs))\n",
    "\n",
    "        # for each neighbourhood size 0,1,...initial_neighbourhood\n",
    "        # build a lookup table where the fraction at neighbourhood_lookup[n,o1,o2]\n",
    "        # indicates if (and how much) weight at index o2 is a neighbour of the weight at index o1 in neighbourhood size n\n",
    "        # use 1 and 0 for a binary mask, or between -1.0 and 1.0 for a varying mask\n",
    "        for neighbourhood in range(0, self.initial_neighbourhood + 1):\n",
    "            nsq = neighbourhood ** 2\n",
    "            for i in range(self.nr_outputs):\n",
    "                ix, iy = self.coords(i)\n",
    "                for j in range(self.nr_outputs):\n",
    "                    jx, jy = self.coords(j)\n",
    "                    sqdist = (jx - ix) ** 2 + (jy - iy) ** 2\n",
    "                    self.neighbourhood_lookup[neighbourhood, i, j] = 1 if sqdist <= nsq else 0\n",
    "\n",
    "#     def get_weights(self, outputIndex):\n",
    "#         return self.weights[:, outputIndex].tolist()\n",
    "\n",
    "    def fit_transform(self, original_instances):\n",
    "\n",
    "        # mask out instances containing NaNs and remove them\n",
    "        instance_mask = ~np.any(np.isnan(original_instances), axis=1)\n",
    "        nr_original_instances = original_instances.shape[0]\n",
    "        valid_instances = original_instances[instance_mask, :]\n",
    "\n",
    "        # randomly re-shuffle the remaining instances.\n",
    "        # TODO consider reshuffling after every iteration\n",
    "        instances = np.copy(valid_instances)\n",
    "        #rng = np.random.default_rng(seed=self.seed)\n",
    "        #rng.shuffle(instances)\n",
    "        np.random.seed(seed=self.seed)\n",
    "        np.random.shuffle(instances)\n",
    "\n",
    "        nr_inputs = instances.shape[1]\n",
    "        nr_instances = instances.shape[0]\n",
    "\n",
    "        weights = np.zeros((self.nr_outputs, nr_inputs))\n",
    "        for output_idx in range(0, self.nr_outputs):\n",
    "            weights[output_idx, :] = instances[self.rng.choice(range(0, nr_instances)), :]\n",
    "\n",
    "#         p = Progress(\"SOM\", silent=not self.verbose)\n",
    "#         progress_frac = 0.0\n",
    "#         p.report(\"Starting\", progress_frac)\n",
    "\n",
    "        for iteration in range(self.iters):\n",
    "            # reduce the learning rate and neighbourhood size linearly as training progresses\n",
    "            learn_rate = self.learn_rate_initial - (self.learn_rate_initial - self.learn_rate_final) * (\n",
    "                        (iteration + 1) / self.iters)\n",
    "            neighbour_limit = round(self.initial_neighbourhood * (1 - (iteration + 1) / self.iters))\n",
    "            neighbourhood_mask = self.neighbourhood_lookup[neighbour_limit, :, :]\n",
    "            batch_size = nr_instances if not minibatch_size else minibatch_size\n",
    "\n",
    "            index = 0\n",
    "            while index < nr_instances:\n",
    "                last_index = min(index + batch_size, nr_instances)\n",
    "                train_batch(instances[index:last_index, :], weights, learn_rate, neighbourhood_mask)\n",
    "                index += batch_size\n",
    "\n",
    "#             progress_frac = iteration / self.iters\n",
    "#             p.report(\"Training neighbourhood=%d\" % (neighbour_limit), progress_frac)\n",
    "\n",
    "#         p.complete(\"SOM Training Complete\")\n",
    "\n",
    "        # compute final scores from the trained weights\n",
    "        valid_scores = compute_scores(valid_instances, weights, self.gridwidth, self.minibatch_size)\n",
    "\n",
    "        # restore the results into the same order as the input array\n",
    "        scores = np.zeros(shape=(nr_original_instances, 2))\n",
    "        scores[:, :] = np.nan\n",
    "        scores[instance_mask, :] = valid_scores\n",
    "        return scores\n",
    "\n",
    "    def coords(self, output):\n",
    "        return (output % self.gridwidth, output // self.gridwidth)\n",
    "\n",
    "#     def get_output(self, x, y):\n",
    "#         return x + (y * self.gridwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca6811fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 ms ± 498 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=1000)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da8c9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 ms ± 226 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(gridsize, gridsize, iters, seed=1, verbose=True, minibatch_size=None)\n",
    "%timeit scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530fbaa",
   "metadata": {},
   "source": [
    "These changes do not hinder the timings, in fact they are very slightly faster, due to the `cupyx.scatter_add` change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0acbb",
   "metadata": {},
   "source": [
    "## Errors for larger gridsizes\n",
    "\n",
    "We can continue to benefit from these increases in speed for larger grid sizes, but as we approach grid size of around 100 x 100, errors occur. For grids of around 200 x 200 we get a memory error, but for 100 x 100 we see a different error. I think this still may be a memory issue as the line it is complaining about shouldn't be a problem. Further investigation needed to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99f1f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 990 ms, sys: 428 ms, total: 1.42 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(50, 50, iters, seed=1, verbose=True, minibatch_size=None)\n",
    "%time scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14711a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "CompileException",
     "evalue": "/opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(185): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(186): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(202): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(207): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(12): error: no operator \"[]\" matches these operands\n            operand types are: CArray<double, 0, true, false> [ const ptrdiff_t * ]\n\n5 errors detected in the compilation of \"/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu\".\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNVRTCError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, options, log_stream)\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddAddNameExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompileProgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/libs/nvrtc.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.compileProgram\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/libs/nvrtc.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.compileProgram\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/libs/nvrtc.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNVRTCError\u001b[0m: NVRTC_ERROR_COMPILATION (6)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCompileException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27350/2761505375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelfOrganisingMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scores = s.fit_transform(instances)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27350/3286095510.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gridwidth, gridheight, iters, initial_neighbourhood, verbose, seed, minibatch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mjx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0msqdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0miy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbourhood_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbourhood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msqdist\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnsq\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m#     def get_weights(self, outputIndex):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__setitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_routines_indexing.pyx\u001b[0m in \u001b[0;36mcupy.core._routines_indexing._ndarray_setitem\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_routines_indexing.pyx\u001b[0m in \u001b[0;36mcupy.core._routines_indexing._scatter_op\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.fill\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel.ElementwiseKernel.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel.ElementwiseKernel._get_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_util.pyx\u001b[0m in \u001b[0;36mcupy._util.memoize.decorator.ret\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel._get_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel._get_simple_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy.core._kernel._get_simple_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.compile_with_cache\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_with_cache\u001b[0;34m(source, options, arch, cache_dir, extra_source, backend, enable_cooperative_groups, name_expressions, log_stream)\u001b[0m\n\u001b[1;32m    368\u001b[0m             name_expressions, log_stream, cache_in_memory)\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         return _compile_with_cache_cuda(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0menable_cooperative_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_expressions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36m_compile_with_cache_cuda\u001b[0;34m(source, options, arch, cache_dir, extra_source, backend, enable_cooperative_groups, name_expressions, log_stream, cache_in_memory)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nvrtc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mcu_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcache_in_memory\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.cu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         ptx, mapping = compile_using_nvrtc(\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcu_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_expressions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             log_stream, cache_in_memory)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_using_nvrtc\u001b[0;34m(source, options, arch, filename, name_expressions, log_stream, cache_in_memory)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mcu_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return _compile(source, options, cu_path,\n\u001b[0m\u001b[1;32m    212\u001b[0m                             name_expressions, log_stream)\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(source, options, cu_path, name_expressions, log_stream)\u001b[0m\n\u001b[1;32m    193\u001b[0m                              name_expressions=name_expressions)\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mptx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCompileException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             dump = _get_bool_env_variable(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, options, log_stream)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNVRTCError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetProgramLog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             raise CompileException(log, self.src, self.name, options,\n\u001b[0m\u001b[1;32m    561\u001b[0m                                    'nvrtc' if not runtime.is_hip else 'hiprtc')\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCompileException\u001b[0m: /opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(185): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(186): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(202): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/opt/conda/lib/python3.8/site-packages/cupy/core/include/cupy/carray.cuh(207): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(8): here\n\n/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu(12): error: no operator \"[]\" matches these operands\n            operand types are: CArray<double, 0, true, false> [ const ptrdiff_t * ]\n\n5 errors detected in the compilation of \"/tmp/tmpefcus7te/28e7b606c3dd6810485fbd9d6fdb1c5d_2.cubin.cu\".\n"
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(100, 100, iters, seed=1, verbose=True, minibatch_size=None)\n",
    "%time scores = s.fit_transform(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7e9e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 76,800,000,000 bytes (allocated so far: 10,831,452,672 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27350/1460532783.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelfOrganisingMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scores = s.fit_transform(instances)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27350/3286095510.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gridwidth, gridheight, iters, initial_neighbourhood, verbose, seed, minibatch_size)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_rate_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_rate_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbourhood_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_neighbourhood\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnr_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnr_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# for each neighbourhood size 0,1,...initial_neighbourhood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/cupy/_creation/basic.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemset_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/core/core.pyx\u001b[0m in \u001b[0;36mcupy.core.core.ndarray.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Out of memory allocating 76,800,000,000 bytes (allocated so far: 10,831,452,672 bytes)."
     ]
    }
   ],
   "source": [
    "s = SelfOrganisingMap(200, 200, iters, seed=1, verbose=True, minibatch_size=None)\n",
    "%time scores = s.fit_transform(instances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
