{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df28fbe",
   "metadata": {},
   "source": [
    "# Getting started with Numba and CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64750fb4",
   "metadata": {},
   "source": [
    " <a href=\"https://numba.readthedocs.io/en/stable/index.html\"><img src=\"images/numba_logo.png\" width=\"200\" /></a>    <a href=\"https://docs.cupy.dev/en/stable/index.html\"><img src=\"images/cupy_logo.png\" width=\"200\" /></a>\n",
    "\n",
    "In this notebook we will be exploring how to use Numba and CuPy to enhance the performance of your Python code. It will take about an hour to run through all the examples in the notebook. We start with some simple examples that cover the basic features of Numba and CuPy. You may wish to skip the final example, _Accelerating Numpy's nanpercentile function_, which is a bit more advanced.\n",
    "\n",
    "__Numba__ is a just-in-time compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python. It generates optimized machine code for the CPU and for GPU hardware. It works especially well on math-heavy Python code that uses NumPy arrays and functions, and loops, which Numba can optimize to performance similar to C, C++ and Fortran.\n",
    "\n",
    "__CuPy__ is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python. CuPy acts as a drop-in replacement to run existing NumPy/SciPy code on NVIDIA CUDA or AMD ROCm platforms. It provides an `ndarray`, sparse matrices, and the associated routines for GPU devices, all having the same API as NumPy and SciPy. Routines are backed by CUDA libraries (cuBLAS, cuFFT, cuSPARSE, cuSOLVER, cuRAND), Thrust, CUB, and cuTENSOR to provide the best performance. It is also possible to easily implement custom CUDA kernels that work with `ndarray`.\n",
    "\n",
    "This notebook will not cover all of the many features of Numba and CuPy, but it is designed to give you a basic introduction to the types of things you can do with these tools and how they can improve the performance of your code.\n",
    "\n",
    "For a comprehensive guide to all things Numba and CuPy, you can check out their official documentation:\n",
    " * [Numba Documentation](https://numba.readthedocs.io/en/stable/index.html)\n",
    " * [CuPy Documentation](https://docs.cupy.dev/en/stable/index.html)\n",
    "\n",
    "In the first section of this notebook we will look at how Numba can be used to optimize code for the CPU. The remaining sections will show you how you can take this knowledge a bit further and use Numba and/or CuPy to accelerate your code using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2422a45",
   "metadata": {},
   "source": [
    "## Topics\n",
    "- [Introduction to Numba](#using_numba)\n",
    "- [Numba for CUDA GPUs](#numba_gpu)\n",
    "- [Introduction to CuPy](#cupy)\n",
    "- [Using Numba and CuPy together](#numba_cupy)\n",
    "- [Accelerating Numpy's nanpercentile function](#nanpercentile)\n",
    "- [Next steps](#next_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f1f76",
   "metadata": {},
   "source": [
    "<a id=\"using_numba\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975d3f0",
   "metadata": {},
   "source": [
    "## Introduction to Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b846f9",
   "metadata": {},
   "source": [
    "The most common way to use Numba is through its collection of decorators. A decorator is a function that takes another function and extends its behaviour without explicitly modifying it. For instance adding some extra steps before or after the function is run. Python allows us to apply decorators more easily using the `@` symbol.\n",
    "\n",
    "For example, rather than applying a decorator function to another function like this:\n",
    "```python\n",
    "def some_func():\n",
    "    print('hello!')\n",
    "\n",
    "some_func = my_decorator_function(some_func)\n",
    "```\n",
    "we can decorate the function directly using the `@` symbol like this:\n",
    "```python\n",
    "@my_decorator_function\n",
    "def some_func():\n",
    "    print('hello!')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482444ca",
   "metadata": {},
   "source": [
    "### Compiling Python code with `@jit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2d98c",
   "metadata": {},
   "source": [
    "Numba's central feature is the `numba.jit()` decorator. Using this decorator, you can mark a function for optimization by Numba’s JIT compiler. \n",
    "\n",
    "Let's import `jit` and take a look at a very simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adfab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeefde0",
   "metadata": {},
   "source": [
    "The following function simply adds together two variables. It has been decorated with the `@jit` decorator. There are various options that can be passed via the decorator, but the recommended way to use it is to let Numba decide when and how to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7567d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def f(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3b8e2",
   "metadata": {},
   "source": [
    "The code will be compiled the first time the function is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76131258",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fba48d",
   "metadata": {},
   "source": [
    "Numba compiled functions can call other compiled functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1081821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@jit\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    return math.sqrt(square(x) + square(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb2fac",
   "metadata": {},
   "source": [
    "The `@jit` decorator must be added to any function that you call within another 'JITed' function, otherwise Numba is likely to generate much slower code. You will also get a warning that compilation is falling back to `object` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    return math.sqrt(square(x) + square(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63428bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypot(1, 2)  # will raise a warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698afac",
   "metadata": {},
   "source": [
    "The recommended best practice is to use `@jit(nopython=True)` or its alias `@njit`, which make it such that only `nopython` mode is used and if compilation fails an exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "@njit\n",
    "def hypot(x, y):\n",
    "    return math.sqrt(square(x) + square(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21818cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypot(1, 2)  # will raise an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb5502",
   "metadata": {},
   "source": [
    "You can find out more about the two compilation modes [here](https://numba.pydata.org/numba-doc/latest/user/5minguide.html#what-is-nopython-mode) in the documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0f593",
   "metadata": {},
   "source": [
    "### Creating NumPy universal functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5a974",
   "metadata": {},
   "source": [
    "There are two types of universal functions:\n",
    "\n",
    "* Those which operate on scalars, these are “universal functions” or ufuncs (see `@vectorize` below).\n",
    "* Those which operate on higher dimensional arrays and scalars, these are “generalized universal functions” or gufuncs (see `@guvectorize` below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050f0ae",
   "metadata": {},
   "source": [
    "#### The `@vectorize` decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16346c57",
   "metadata": {},
   "source": [
    "Using Numba’s `@vectorize` decorator, you can write a function that operates on scalar inputs and use it as a NumPy ufunc. Numba will generate the surrounding loop so you can pass arrays as inputs. A pure Python function can be compiled to operate on NumPy arrays as fast as a ufunc written in C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77071244",
   "metadata": {},
   "source": [
    "Let's take our simple addition example and decorate it with `@vectorize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "\n",
    "@vectorize\n",
    "def f(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32a736",
   "metadata": {},
   "source": [
    "Our function takes scalar inputs, but we can pass it NumPy arrays and it will perform the addition for each element in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(12).reshape(3, 4)\n",
    "b = np.arange(12).reshape(3, 4)\n",
    "\n",
    "f(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf502a",
   "metadata": {},
   "source": [
    "Numba will also handle a mix of arrays and scalars or arrays of different shapes if they can be broadcast to the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(3, 4)\n",
    "b = 10\n",
    "\n",
    "f(a, b)  # 10 will be added to each element of a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c70843",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(3, 4)\n",
    "b = np.arange(4).reshape(1, 4)\n",
    "\n",
    "f(a, b)  # b will be broadcast to a (3, 4) array and added to a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f8a05",
   "metadata": {},
   "source": [
    "#### The `@guvectorize` decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02940d1",
   "metadata": {},
   "source": [
    "We can extend the behaviour of `vectorize()` to write ufuncs that work on an arbitrary number of elements of your input arrays. The `guvectorize()` decorator allows you to use arrays of differing dimensions for the inputs and outputs of your functions.\n",
    "\n",
    "When you use the `guvectorize()` decorator, you should include the output array as an argument to your function and use the function to fill in this array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3104a",
   "metadata": {},
   "source": [
    "To demonstrate, we shall work with another addition example, but this time the function will involve adding a scalar to all elements in a 1D array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, y, res):\n",
    "    for i in range(x.shape[0]):\n",
    "        res[i] = x[i] + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ee164",
   "metadata": {},
   "source": [
    "When we decorate this function with `guvectorize()` we should include the type signatures for the inputs and output, as well as a declaration of input and output layouts, in symbolic form. For this declaration an empty tuple is used to represent a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9950c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import guvectorize\n",
    "\n",
    "@guvectorize(['int64[:], int64, int64[:]'], '(n),()->(n)')\n",
    "def g(x, y, res):\n",
    "    for i in range(x.shape[0]):\n",
    "        res[i] = x[i] + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e5a35",
   "metadata": {},
   "source": [
    "In the above decorator we tell Numba that will be operating on a 1D array of type `int64` (`int64[:]`) and a scalar of type `int64` and will produce a 1D output array of type `int64`. The declaration of input and output layouts tells Numba that an input array with shape `(n)` and a scalar will be operated on to produce an output array of the same shape as the first input, `(n)`.\n",
    "\n",
    "We can therefore run the function with a 1D array and a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(5)\n",
    "b = 2\n",
    "\n",
    "g(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb75fe",
   "metadata": {},
   "source": [
    "With the `guvectorize()` decorator in place, Numba automatically applies this operation over more complicated input shapes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c46141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D array and scalar\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "b = 2\n",
    "\n",
    "g(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb82e",
   "metadata": {},
   "source": [
    "Above, Numba applies the operation defined in the function for the two 1D array and scalar inputs contained within the problem, i.e.\n",
    "```python\n",
    "array([0, 1, 2]) + 2\n",
    "```\n",
    "and\n",
    "```python\n",
    "array([3, 4, 5]) + 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b507eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D array and broadcastable array\n",
    "a = np.arange(6).reshape(2, 3)\n",
    "b = np.array([10, 20])\n",
    "\n",
    "g(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17205550",
   "metadata": {},
   "source": [
    "Similarly above, the function is applied to\n",
    "```python\n",
    "array([0, 1, 2]) + 10\n",
    "```\n",
    "and\n",
    "```python\n",
    "array([3, 4, 5]) + 20\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cde09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass an empty array to use for output\n",
    "res = np.empty_like(a)\n",
    "g(a, b, res)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3993c4",
   "metadata": {},
   "source": [
    "### Example: Applying NDVI to Sentinel 2 data\n",
    "\n",
    "Now we'll take a look at an example where the methods described above could be used to speed up an operation in Python. Two sample files are provided: `T47NQD_20190812T032541_B04.jp2` and `T47NQD_20190812T032541_B08.jp2` that contain the red and near infrared bands for a Sentinel 2 scene. We'll be working with an image of the Klang Islands in Malaysia. We can use these files to produce the Normalized Difference Vegetation Index (NDVI) for the scene via the formula:\n",
    "\n",
    "NDVI = (NIR-RED) / (NIR+RED).\n",
    "\n",
    "First we'll import the required libraries for this example and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a69bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_file = 'http://data.neodaas.ac.uk/files/getting_started_with_numba_and_cupy/T47NQD_20190812T032541_B04.jp2'\n",
    "with rasterio.open(red_file) as src:\n",
    "    red_array = src.read(out_shape=(\n",
    "        src.count, int(src.height), int(src.width)))\n",
    "red_array = red_array[0]\n",
    "\n",
    "nir_file = 'http://data.neodaas.ac.uk/files/getting_started_with_numba_and_cupy/T47NQD_20190812T032541_B08.jp2'\n",
    "with rasterio.open(nir_file) as src:\n",
    "    nir_array = src.read(out_shape=(\n",
    "        src.count, int(src.height), int(src.width)))\n",
    "nir_array = nir_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defb715",
   "metadata": {},
   "source": [
    "Now let's take a quick look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26496043",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_array.shape, nir_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449b0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[16, 8])\n",
    "axs[0].imshow(red_array)\n",
    "axs[0].set_title('Red band')\n",
    "axs[1].imshow(nir_array)\n",
    "axs[1].set_title('NIR band')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a6cdd",
   "metadata": {},
   "source": [
    "A simple approach to calculating the index for the entire scene would be to loop over all of the pixels for each of the bands and apply the formula to calculate NDVI at each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_array = red_array.astype(np.float64)\n",
    "nir_array = nir_array.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ndvi_res = np.empty_like(red_array)\n",
    "for i in range(ndvi_res.shape[0]):\n",
    "    for j in range(ndvi_res.shape[1]):\n",
    "        ndvi_res[i, j] = (nir_array[i, j] - red_array[i, j]) / (nir_array[i, j] + red_array[i, j])\n",
    "        \n",
    "# This will take a few minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93508b2",
   "metadata": {},
   "source": [
    "The calculations in this form take around 4-5 minutes to run because we are operating on one pixel at a time. Re-writing the above function in a vectorized way will significantly reduce the time it takes to run.\n",
    "\n",
    "Since we are working with NumPy arrays we can do the above calculation with NumPy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_res_vec = (nir_array - red_array) / (nir_array + red_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc16c8",
   "metadata": {},
   "source": [
    "and comparing timings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd68548",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ndvi_res_vec = (nir_array - red_array) / (nir_array + red_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68c2b3",
   "metadata": {},
   "source": [
    "We see that it runs over 200 times faster!\n",
    "\n",
    "Now that the code has been written in this vectorized format, it is easy to add the vectorize decorator and compile with Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def ndvi(red, nir):\n",
    "    return (nir - red) / (nir + red)\n",
    "\n",
    "@vectorize\n",
    "def get_ndvi(red, nir):\n",
    "    return ndvi(red, nir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_res_numba = get_ndvi(red_array, nir_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e278937",
   "metadata": {},
   "source": [
    "Comparing timings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ndvi_res_numba = get_ndvi(red_array, nir_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8402aa",
   "metadata": {},
   "source": [
    "So we are able to cut the time in half again using Numba.\n",
    "\n",
    "Let's take a look at the results to ensure we are still getting the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=[16, 8])\n",
    "axs[0].imshow(ndvi_res)\n",
    "axs[0].set_title('NDVI using loops')\n",
    "axs[1].imshow(ndvi_res_vec)\n",
    "axs[1].set_title('NDVI using NumPy')\n",
    "axs[2].imshow(ndvi_res_numba)\n",
    "axs[2].set_title('NDVI using Numba')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d548d1",
   "metadata": {},
   "source": [
    "You now have enough information to potentially speed up many of your existing workflows. In some cases it will be as simple as importing `numba` and adding the `@jit` decorator to some of your functions.\n",
    "\n",
    "Read on to see how you can benefit from running your code on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46057708",
   "metadata": {},
   "source": [
    "<a id=\"numba_gpu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451bad43",
   "metadata": {},
   "source": [
    "## Numba for CUDA GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6e31f",
   "metadata": {},
   "source": [
    "Numba supports CUDA GPU programming by directly compiling Python code into CUDA kernels and device functions following the CUDA execution model. As with running Numba on the CPU, not all Python code is supported. For CUDA Python, you can see the full list of supported features here: [Supported Python features in CUDA Python](https://numba.pydata.org/numba-doc/latest/cuda/cudapysupported.html)\n",
    "\n",
    "One of the main differences you will see between the supported features running on the CPU vs GPU are the limited NumPy methods supported for the GPU, as Numba disallows any memory allocating features for CUDA kernels.\n",
    "\n",
    "Some key terms for CUDA programming that will be used in this sectoion are listed here:\n",
    " * _host_: the CPU\n",
    " * _device_: the GPU\n",
    " * _kernels_: a GPU function launched by the host and executed on the device\n",
    " * _device function_: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ab8da",
   "metadata": {},
   "source": [
    "We have seen how we can use the `vectorize()` and `guvectorize()` decorators on the CPU to create ufuncs. With a very small modification, these decorators can be used to run functions on a GPU. However, CUDA Vectorize and GUVectorize cannot produce a conventional ufunc and instead return a ufunc-like object. This object is a close analog but not fully compatible with a regular NumPy ufunc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800759f2",
   "metadata": {},
   "source": [
    "### CUDA ufunc-like objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d49c3",
   "metadata": {},
   "source": [
    "Let's take the simple addition function we used to demonstrate `@vectorize` on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize\n",
    "def f(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd10286",
   "metadata": {},
   "source": [
    "As with the CPU example seen earlier, we can use the type signature with this decorator. We can also change the default `target` option. Switching to `target='cuda'` will run our function on a GPU.\n",
    "\n",
    "Below the `float32` outside of the brackets represents the output type and the `float32` types listed within the brackets are the types of each input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def f(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063011cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12, dtype=np.float32).reshape(3, 4)\n",
    "b = np.arange(12, dtype=np.float32).reshape(3, 4)\n",
    "\n",
    "f(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819c717",
   "metadata": {},
   "source": [
    "When using the type signature, we must ensure that we pass the correct variable types to the function. See what happens when we try to pass integer arrays to our function when it is expecting floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633ca7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(3, 4)\n",
    "b = np.arange(12).reshape(3, 4)\n",
    "\n",
    "f(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1a4cb",
   "metadata": {},
   "source": [
    "### Memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fef023",
   "metadata": {},
   "source": [
    "In our above example, Numba is automatically transfering the NumPy arrays to and from the device. The input arrays are copied to the device and arrays are copied back to the host when the kernel finishes. Where the arrays are read-only, this transfer is unnecessary. There may also be cases where you run several kernels, passing the output from one as an input to the next, and only need the output from the final kernel. If we rely on Numba to automatically handle our transfers, there will be extra copying carried out between each kernel that is not needed.\n",
    "\n",
    "You can use the following APIs to manually control the transfer:\n",
    "* `numba.cuda.device_array()` - to create an array on the device\n",
    "* `numba.cuda.to_device()` - to copy a host array to the device\n",
    "\n",
    "In the following example the inputs to our simple addition function are manually copied to the device and an output device array is created before running the function. We then pass the arrays that are already on the device to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4707e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8121a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "a = np.arange(n).astype(np.float32)\n",
    "b = 2 * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49975758",
   "metadata": {},
   "source": [
    "We can compare the time taken to run the function using the automatic array transfers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit f(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d88f0",
   "metadata": {},
   "source": [
    "with the manual copying of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_device = cuda.to_device(a)\n",
    "b_device = cuda.to_device(b)\n",
    "\n",
    "out_device = cuda.device_array(shape=(n,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit f(a, b, out=out_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d1abe",
   "metadata": {},
   "source": [
    "The output can be copied back to the host if/when it is needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_host = out_device.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed4189",
   "metadata": {},
   "source": [
    "As expected the function runs faster when the arrays are already on the device. In this simple example the manual copying does not lead to dramatically faster timings. However there are cases where this type of memory management can have a huge impact on speed. For example when looping over a kernel we can wait until all loops have been completed before copying back a final result, rather than carrying out slow tranfers in every loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3284f62",
   "metadata": {},
   "source": [
    "### Example: Applying NDVI to Sentinel 2 data using a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db794229",
   "metadata": {},
   "source": [
    "We saw in our ealier CPU examples that functions with the `@jit` decorator can be used within other JITed functions or vectorized functions. The same is true when running on GPUs, but in this case we need to use the `@cuda.jit` decorator and turn it into a device function using the `device=True` option. CUDA device functions can only be called from other device functions or via a kernel.\n",
    "\n",
    "Using the NDVI example, we can demonstrate the changes needed to go from running on the CPU to running on a GPU.\n",
    "\n",
    "Below we will replace the `@njit` decorator with the `@cuda.jit` decorator. We will also add the `target='cuda'` option to the vectorize decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc70432",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def ndvi(red, nir):\n",
    "    return (nir - red) / (nir + red)\n",
    "\n",
    "@vectorize(['float64(float64, float64)'], target='cuda')\n",
    "def get_ndvi(red, nir):\n",
    "    return ndvi(red, nir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19014f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_device = cuda.to_device(red_array)\n",
    "nir_device = cuda.to_device(nir_array)\n",
    "\n",
    "out_device = cuda.device_array_like(red_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebbceb",
   "metadata": {},
   "source": [
    "We have created an output array to pass to the function, which the result will be written to on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac93529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_ndvi(red_device, nir_device, out=out_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cd85a",
   "metadata": {},
   "source": [
    "The result can be copied back to the host ready for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_host = out_device.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf52639",
   "metadata": {},
   "source": [
    "Let's confirm that we get the same results when running on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=[16, 8])\n",
    "axs[0].imshow(ndvi_res)\n",
    "axs[0].set_title('NDVI on CPU')\n",
    "axs[1].imshow(out_host)\n",
    "axs[1].set_title('NDVI on GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb09d74",
   "metadata": {},
   "source": [
    "Have we gained any increase in speed by moving the processing to the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_ndvi(red_device, nir_device, out=out_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a7ccc",
   "metadata": {},
   "source": [
    "The function now runs in about 4ms. That's over 100 times faster than using Numba on the CPU and over 50,000 times as fast as the original looping solution!\n",
    "\n",
    "You can actually speed things up further by converting the data to `float32` from `float64` which runs slowly on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eff705",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def get_ndvi(red, nir):\n",
    "    return ndvi(red, nir)\n",
    "\n",
    "red_array = red_array.astype(np.float32)\n",
    "nir_array = nir_array.astype(np.float32)\n",
    "\n",
    "red_device = cuda.to_device(red_array)\n",
    "nir_device = cuda.to_device(nir_array)\n",
    "\n",
    "out_device = cuda.device_array_like(red_device)\n",
    "# Run the function once before timing it to allow Numba to compile the function.\n",
    "get_ndvi(red_device, nir_device, out=out_device)\n",
    "\n",
    "%timeit get_ndvi(red_device, nir_device, out=out_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb3b93",
   "metadata": {},
   "source": [
    "<a id=\"cupy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9bba7",
   "metadata": {},
   "source": [
    "## Introduction to CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2e263",
   "metadata": {},
   "source": [
    "We will now look at another useful tool for porting code to GPUs - CuPy. This is the perfect tool to use if you make a lot of use of NumPy in your code. If you know how to use NumPy, you can use CuPy. In theory you can port your code to run on a GPU by changing a single line of code. There are some differences between NumPy and CuPy as discussed in the documentation: [Difference between CuPy and NumPy](https://docs.cupy.dev/en/stable/user_guide/difference.html) and you may need to make some minor modifications to your code to get it to run with CuPy instead of NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48676693",
   "metadata": {},
   "source": [
    "The convention for importing CuPy is to use the abbreviation `cp`, just as we import `numpy` as `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da242f",
   "metadata": {},
   "source": [
    "CuPy has an `ndarray` class that replaces Numpy's `ndarray` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3])\n",
    "x_gpu = cp.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dfcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_cpu))\n",
    "print(type(x_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbbcae",
   "metadata": {},
   "source": [
    "We can also apply many of the same functions to the CuPy arrays as we would typically use with a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(x_cpu))\n",
    "print(cp.average(x_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b3d3a",
   "metadata": {},
   "source": [
    "The main difference between the two arrays is that the CuPy array is stored on the _current device_, which is the default GPU device where the array operations take place. The NumPy array is stored on the host."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c098cd",
   "metadata": {},
   "source": [
    "If you need to move NumPy or CuPy arrays to the same location the methods `cupy.asarray()` and `cupy.asnumpy()` are available to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99764a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3])\n",
    "x_gpu = cp.asarray(x_cpu)  # move the data to the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_cpu))\n",
    "print(type(x_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b426e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array([1, 2, 3])  # create an array on the current device\n",
    "x_cpu = cp.asnumpy(x_gpu)  # move the array to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_cpu))\n",
    "print(type(x_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09fb85",
   "metadata": {},
   "source": [
    "Where the two libraries share the same API, it is possible to port a piece of code to run on a GPU by changing one line of code, e.g. the following piece of code uses NumPy to carry out some basic operations on ndarrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "\n",
    "a = np.arange(n*n).reshape(n, n)\n",
    "b = np.ones_like(a)\n",
    "\n",
    "c = np.sqrt(a + b)\n",
    "c_avg = np.average(c, axis=0)\n",
    "\n",
    "max_avg = np.max(c_avg)\n",
    "\n",
    "print(max_avg)\n",
    "print(type(max_avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30dd8d2",
   "metadata": {},
   "source": [
    "Let's time how long it takes to complete these operations with Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7909c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "n = 1000\n",
    "\n",
    "a = np.arange(n*n).reshape(n, n)\n",
    "b = np.ones_like(a)\n",
    "\n",
    "c = np.sqrt(a + b)\n",
    "c_avg = np.average(c, axis=0)\n",
    "\n",
    "max_avg = np.max(c_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc2792",
   "metadata": {},
   "source": [
    "By importing CuPy as `np` we can run the same code on a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f7201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np  # changing this line only allows the code to run on a GPU\n",
    "\n",
    "n = 1000\n",
    "\n",
    "a = np.arange(n*n).reshape(n, n)\n",
    "b = np.ones_like(a)\n",
    "\n",
    "c = np.sqrt(a + b)\n",
    "c_avg = np.average(c, axis=0)\n",
    "\n",
    "max_avg = np.max(c_avg)\n",
    "\n",
    "print(max_avg)\n",
    "print(type(max_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "n = 1000\n",
    "\n",
    "a = np.arange(n*n).reshape(n, n)\n",
    "b = np.ones_like(a)\n",
    "\n",
    "c = np.sqrt(a + b)\n",
    "c_avg = np.average(c, axis=0)\n",
    "\n",
    "max_avg = np.max(c_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7be44a",
   "metadata": {},
   "source": [
    "By changing this one line of code, we are able to run the same calculations about 40 times as fast. Note that the results running on the CPU vs GPU are slightly different (see last digit). In some cases the underlying methods used by NumPy and CuPy will differ leading to slight differences in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859d55d",
   "metadata": {},
   "source": [
    "### CuPy Custom Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4a2ad",
   "metadata": {},
   "source": [
    "As well as using CuPy as a drop in replacement for NumPy to run on a GPU, there are a number of other useful tools that come with CuPy including methods for writing user-defined CUDA kernels. Writing your own CUDA kernel can be complicated, so CuPy provides easy ways to define three types of CUDA kernels: elementwise kernels, reduction kernels and raw kernels. Below we look at an example of using CuPy's `ElementwiseKernel` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90144363",
   "metadata": {},
   "source": [
    "To define an elementwise kernel we need to pass four arguments to `cp.ElementwiseKernel()`:\n",
    " * an input argument list,\n",
    " * an output argument list,\n",
    " * a loop body code,\n",
    " * and the kernel name.\n",
    " \n",
    "Each of these arguments is a string. The input argument list and output argument list both consist of comma-separated argument definitions. These definitions are made up of the argument type and the argument name. The loop body code defines how the output is calculated as a function of the inputs.\n",
    "\n",
    "For example, a kernel that computes a squared difference is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_diff = cp.ElementwiseKernel(\n",
    "   'float32 x, float32 y',\n",
    "   'float32 z',\n",
    "   'z = (x - y) * (x - y)',\n",
    "   'squared_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0437f9",
   "metadata": {},
   "source": [
    "The above kernel takes two inputs, `x` and `y`, that are both of type `float32`, and returns the `float32` output `z`. It can be called on either scalars or arrays with broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e003b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cp.arange(10, dtype=cp.float32).reshape(2, 5)\n",
    "y = 5\n",
    "\n",
    "squared_diff(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668067b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cp.arange(10, dtype=np.float32).reshape(2, 5)\n",
    "y = cp.arange(5, dtype=np.float32)\n",
    "\n",
    "squared_diff(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d392f0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"numba_cupy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f8797",
   "metadata": {},
   "source": [
    "## Using Numba and CuPy together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2dbd69",
   "metadata": {},
   "source": [
    "`cupy.ndarray` implements `__cuda_array_interface__`, which is the CUDA array interchange interface compatible with Numba v0.39.0 or later. This means you can pass CuPy arrays to kernels JITed with Numba. Using CuPy arrays with Numba can simplify the process of copying arrays to and from the device.\n",
    "\n",
    "Again we will use the NDVI example, this time to show how Numba and CuPy can be used together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc51c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from numba import cuda, vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from earlier example:\n",
    "@cuda.jit(device=True)\n",
    "def ndvi(red, nir):\n",
    "    return (nir - red) / (nir + red)\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def get_ndvi(red, nir):\n",
    "    return ndvi(red, nir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our numpy input arrays to cupy arrays and create cupy array for output:\n",
    "gpu_red = cp.asarray(red_array, dtype=cp.float32)\n",
    "gpu_nir = cp.asarray(nir_array, dtype=cp.float32)\n",
    "gpu_out = cp.zeros_like(gpu_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fef198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass these cupy arrays, that exist on the device, to the kernel:\n",
    "get_ndvi(gpu_red, gpu_nir, out=gpu_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe511414",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gpu_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f5ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert output to a numpy array ready for plotting:\n",
    "cpu_out = cp.asnumpy(gpu_out)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[16, 8])\n",
    "ax.imshow(cpu_out)\n",
    "ax.set_title('NDVI with Numba and CuPy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_ndvi(gpu_red, gpu_nir, out=gpu_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e10cb",
   "metadata": {},
   "source": [
    "Using CuPy with the Numba vectorized function still gives us similar times as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903f800",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"nanpercentile\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39bf88",
   "metadata": {},
   "source": [
    "## Accelerating Numpy's nanpercentile function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df030f",
   "metadata": {},
   "source": [
    "In this section we will take what we have learned in the previous sections and apply it to a common operation that is known to be slow for large datasets.\n",
    "\n",
    "If you have ever had to calculate percentiles for a dataset containing NaNs, you may well have used NumPy's `nanpercentile()` function. For smaller datasets it can work well, but as the dataset grows this function can take a very long time to run.\n",
    "\n",
    "There was a blog post a few years ago that looked at this very problem, which you can find [here](https://krstn.eu/np.nanpercentile()-there-has-to-be-a-faster-way/). In this blog they proposed an alternative pure Python implementation of the quantile calculation with the following steps:\n",
    " * find the number of valid observations (non NaN)\n",
    " * replace NaN with maximum value of array\n",
    " * sort values along axis\n",
    " * find position of quantile regarding number of valid observations\n",
    " * linear interpolation if the desired quantile is inbetween two positions (like numpys linear interpolation)\n",
    "\n",
    "You can find the code from this blog in the `nanpercentile_functions.ipynb` notebook, which we will import below to allow us to compare timings.\n",
    "\n",
    "We can use a similar approach to speed things up with a GPU using Numba and CuPy, and in particular the `guvectorize` decorator. To demonstrate this we'll generate a test array with shape 20 x 500 x 500 and calculate a 500 x 500 array of the 90th percentile values.\n",
    "\n",
    "First we'll import all the required modules and functions that are needed in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from numba import cuda, njit, guvectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the nanpercentile_functions notebook so that we have access to the gpu_quickSortIterative function\n",
    "# to be used in our GPU solution and the nan_percentile function used in the blog:\n",
    "%run nanpercentile_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image of size 500x500 with 20 layers, ie. array of shape (20,500,500)\n",
    "test_arr = np.random.randint(0, 250000, 5000000).reshape(20,500,500).astype(np.float32)\n",
    "np.random.shuffle(test_arr)\n",
    "# place random NaN\n",
    "rand_nan = np.random.randint(0, 5000000, 5000).astype(np.float32)\n",
    "for r in rand_nan:\n",
    "    test_arr[test_arr == r] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a9ec4",
   "metadata": {},
   "source": [
    "Let's see how long it takes to calculate the 90th percentiles for the test array using Numpy's `nanpercentile` function. You may wish to try different sized arrays above to see the impact it can have on run times, or read in some of your own real-life data to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr = np.array(test_arr, copy=True)\n",
    "numpy_nan = np.nanpercentile(input_arr, q=[90], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "input_arr = np.array(test_arr, copy=True)\n",
    "np.nanpercentile(input_arr, q=[90], axis=0)\n",
    "# this will take a few minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b982a2",
   "metadata": {},
   "source": [
    "We can see that the pure Python alternative described by the blog post does a good job of speeding things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16704f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr = np.array(test_arr, copy=True)\n",
    "cpu_blog_nan = nan_percentile(input_arr, q=[90])  # nan_percentile is defined in the nanpercentile_functions.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aece37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "input_arr = np.array(test_arr, copy=True)\n",
    "nan_percentile(input_arr, q=[90])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ec59e",
   "metadata": {},
   "source": [
    "Let us now apply what we have learned in this notebook to see if we can do any better with a GPU. We will write a pure Python implementation of the 90th percentile calculation. This could be extended to calculate arbitrary percentiles, but for the purposes of this notebook we shall keep things as simple as possible.\n",
    "\n",
    "We will make use of the `guvectorize` decorator. The operation being perfomed by the decorated function will be to calculate the 90th percentile of a 1 dimensional array. By using the `guvectorize`, we can pass higher dimensional arrays to the function and Numba will handle running the operation across all dimensions.\n",
    "\n",
    "The main function will also make use of 2 device functions:\n",
    "* `gpu_quickSortIterative` - this is defined in `nanpercentile_functions.ipynb` and used to sort the 1D array on the device. \n",
    "* `gpu_calc_90_quant` - this is defined below and is used to calculate the quantile values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60804d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def gpu_calc_90_quant(k_arr, f_arr, c_arr, f_val, c_val):\n",
    "    '''\n",
    "    Calculate the 90th quantile.\n",
    "    \n",
    "    Args:\n",
    "     * k_arr - desired quantile position\n",
    "     * f_arr - floor value of k_arr\n",
    "     * c_arr - ceiling value of k_arr\n",
    "     * f_val - value in input array at position [f_arr]\n",
    "     * c_val - value in input array at postion [c_arr]\n",
    "    '''\n",
    "    if f_arr == c_arr:\n",
    "        quant_val = f_val\n",
    "    else:\n",
    "        floor_frac = c_arr - k_arr\n",
    "        floor_frac_val = f_val * floor_frac\n",
    "\n",
    "        ceil_frac = k_arr - f_arr\n",
    "        ceil_frac_val = c_val * ceil_frac\n",
    "\n",
    "        quant_val = floor_frac_val + ceil_frac_val\n",
    "\n",
    "    return quant_val\n",
    "\n",
    "@guvectorize(['float32[:], int32[:], int32[:], float32[:], float32[:]'],\n",
    "             '(p), (i), (i), (i) -> ()', target='cuda')\n",
    "def gpu_get_nan_perc(max_val, stack, ids, arr, quant90):\n",
    "    '''\n",
    "    This function requires the following arguments:\n",
    "    \n",
    "    * max_val - Maximum value of input array\n",
    "    * stack - An array of zeros in the same shape as input array.\n",
    "              It is used by the sorting kernel.\n",
    "    * ids - An array of zeros in the same shape as input array.\n",
    "            It is used by the sorting kernel.\n",
    "    * arr - The input array for which we want to calculate the\n",
    "            90th percentile.\n",
    "            \n",
    "    output: quant90 - a 0D array containing 90th percentile result.\n",
    "    \n",
    "    '''\n",
    "    valid_obs = 0\n",
    "    for i in range(arr.shape[-1]):\n",
    "        if math.isfinite(arr[i]):\n",
    "            # Count the number of valid observations to be used\n",
    "            # to determine quantile position.\n",
    "            valid_obs += 1\n",
    "        else:\n",
    "            # Replace NaNs with the maximum value of the array.\n",
    "            arr[i] = max_val[0]\n",
    "\n",
    "    # Sort the array using device function.\n",
    "    # NaNs, having been replaced with the max value will all be shifted to the\n",
    "    # right of the array.\n",
    "    gpu_quickSortIterative(arr, stack, ids)\n",
    "\n",
    "    # Calculate the inputs for the gpu_calc_90_quant function.\n",
    "    k_arr = (valid_obs - 1) * 0.9\n",
    "    f_arr = int(math.floor(k_arr))\n",
    "    c_arr = int(math.ceil(k_arr))\n",
    "    f_val = arr[f_arr]\n",
    "    c_val = arr[c_arr]\n",
    "\n",
    "    # The output is a zero dimensional array containing the 90th percentile value.\n",
    "    quant90[0] = gpu_calc_90_quant(k_arr, f_arr, c_arr, f_val, c_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab9131",
   "metadata": {},
   "source": [
    "Note that the output is a zero dimensional array and the result must be assigned to the zero index.\n",
    "\n",
    "Our test array has the dimension that we want to calculate the percentiles for as the first axis. `np.nanpercentile` can take an axis argument and handle this. The blog version is specifically written to collapse over the first axis, but not handle doing the calculation over different axes.\n",
    "\n",
    "For the guvectorize solution to work we want the percentiles to be calculated for the last dimension. So we'll need to swap the axis of our input array to apply the function to it (this step could alternatively be carried out in the nan_percentile function with options to choose the axis like NumPy's version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr = np.array(test_arr, copy=True)\n",
    "# Re-order the axes so that we can apply the percentiles calculation over the last dimension\n",
    "input_arr = np.rollaxis(input_arr, 0, 3)\n",
    "# Convert our NumPy array to a CuPy array\n",
    "input_arr = cp.asarray(input_arr)\n",
    "\n",
    "# Make CuPy arrays for remaining inputs\n",
    "my_stack = cp.zeros_like(input_arr, dtype=cp.int32)\n",
    "my_ids = cp.zeros_like(input_arr, dtype=cp.int32)\n",
    "max_val = cp.nanmax(input_arr, axis=2).reshape(input_arr.shape[0], input_arr.shape[1], 1)\n",
    "\n",
    "# Run our function passing the cupy arrays as arguments\n",
    "gpu_nan = gpu_get_nan_perc(max_val, my_stack, my_ids, input_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "input_arr = np.array(test_arr, copy=True)\n",
    "input_arr = np.rollaxis(input_arr, 0, 3)\n",
    "input_arr = cp.asarray(input_arr)\n",
    "my_stack = cp.zeros_like(input_arr, dtype=cp.int32)\n",
    "my_ids = cp.zeros_like(input_arr, dtype=cp.int32)\n",
    "max_val = cp.nanmax(input_arr, axis=2).reshape(input_arr.shape[0], input_arr.shape[1], 1)\n",
    "gpu_get_nan_perc(max_val, my_stack, my_ids, input_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0f7fc",
   "metadata": {},
   "source": [
    "This is much faster than running on the CPU:\n",
    "about 7 x faster than the solution provided in the blog post and over 1000 x faster than Numpy's nanpercentile function!\n",
    "\n",
    "We should confirm that the results are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(numpy_nan, cpu_blog_nan), np.allclose(numpy_nan, gpu_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12af7c9",
   "metadata": {},
   "source": [
    "We have now seen how Numba and CuPy can be used to speed up array operations. As problems become larger these performance gains can become more important e.g. code that takes several hours to run can be run in a matter of seconds. These kind of improvements are often possible for math-heavy code that can be run in parallel.\n",
    "\n",
    "In some cases we can see performance improvements by changing a single line of code. In other cases such as the percentiles problem, the code may need to be re-written to get the benefits of using the GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8263f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"next_steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b5308",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36561c7",
   "metadata": {},
   "source": [
    "The tools covered in this notebook are just a taste of what is possible with Numba and CuPy and there are lots of nice features that you may wish to explore a bit further in the user documentation:\n",
    " * [Numba Documentation](https://numba.readthedocs.io/en/stable/index.html)\n",
    " * [CuPy Documentation](https://docs.cupy.dev/en/stable/index.html)\n",
    "\n",
    "In this notebook we have been using Numba and CuPy to decide for us how to execute our code. For example we haven't had to think about how many threads to run on, or what is running on each thread. For more complicated problems that cannot be written as ufuncs we need to understand a bit more about the CUDA programming model. There are more details about writing CUDA kernels with Numba here:\n",
    "\n",
    "* https://numba.readthedocs.io/en/stable/cuda/kernels.html\n",
    "\n",
    "and writing CUDA kernels with CuPy here:\n",
    "\n",
    "* https://docs.cupy.dev/en/stable/user_guide/kernel.html#raw-kernels\n",
    "\n",
    "You can find out more about the CUDA programming model here:\n",
    "\n",
    "* https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#introduction\n",
    "* https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model\n",
    "\n",
    "Rather than writing your own CUDA kernels, you may want to see if what you need has already been written. You can find some examples of existing libraries that are based on CUDA here:\n",
    "\n",
    "* https://rapids.ai/\n",
    "* https://developer.nvidia.com/gpu-accelerated-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523786d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
